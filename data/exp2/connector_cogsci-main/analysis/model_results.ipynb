{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L_594629nfMU"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AbdIoQI7noEY"
   },
   "outputs": [],
   "source": [
    "import heapq\n",
    "import json\n",
    "import itertools\n",
    "import sys\n",
    "import scipy.spatial.distance\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from scipy import stats\n",
    "from functools import lru_cache\n",
    "from numpy.random import randint\n",
    "from scipy.special import softmax\n",
    "from sklearn.preprocessing import MinMaxScaler, normalize\n",
    "from numpy.linalg import matrix_power\n",
    "from scipy.special import expit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OkXo_Ysanp92"
   },
   "source": [
    "# Import the full vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 182,
     "status": "ok",
     "timestamp": 1626742006345,
     "user": {
      "displayName": "Robert Hawkins",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiqGTtXNDJINdLVVy9iBBFVLUYe9UyQWhEaBwPLqw=s64",
      "userId": "13623832260192960306"
     },
     "user_tz": 420
    },
    "id": "U3iBYEMWnpcY",
    "outputId": "b20683f3-da39-46ec-f134-6a2efb10aae9"
   },
   "outputs": [],
   "source": [
    "sample_df = pd.read_csv(\"../data/vocab.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L2MFRtHgnwIN"
   },
   "source": [
    "# Read in semantic representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have (1) glove, (2) SWOW, and (3) (non-contextual) BERT embeddings.\n",
    "\n",
    "SWOW has 2 versions : PPMI and Random Walk. We use RW.\n",
    "\n",
    "The BERT context-free embeddings obtained by \"CLS [word] SEP\": summed across last four layers (768-dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YZqmDQr-nxik"
   },
   "outputs": [],
   "source": [
    "representations = {}\n",
    "representations['glove'] = pd.read_csv(\"../data/glove_embeddings.csv\").transpose().values\n",
    "representations['swow'] = pd.read_csv(\"../data/swow_embeddings.csv\").transpose().values\n",
    "representations['bert-sum'] = pd.read_csv(\"../data/bert_embeddings.csv\").transpose().values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check vocab size is 12218 words for all 3 representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for representation in representations :\n",
    "    assert representations[representation].shape[0] == 12218"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check embedding sizes are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert representations['glove'].shape[1] == 300\n",
    "assert representations['swow'].shape[1] == 300\n",
    "assert representations['bert-sum'].shape[1] == 768"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p4qERX9poGEj"
   },
   "source": [
    "# Read in the game boards used in the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j3EntMS7oHJd"
   },
   "outputs": [],
   "source": [
    "with open('../data/boards.json', 'r') as json_file:\n",
    "    boards = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9uHvyQXJoSns"
   },
   "source": [
    "# Read in the clues produced in the experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expdata = pd.read_csv(\"../data/final_board_clues_all.csv\", encoding= 'unicode_escape')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MBiSuy_poVpI"
   },
   "source": [
    "# RSA functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T6VJDvQAdoQA"
   },
   "source": [
    "### Extract wordpairs\n",
    "For each board, we need to get all of the possible pairs of words on the board in an easy-to-work-with format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JjlD7j9oo8sH"
   },
   "outputs": [],
   "source": [
    "## these combos need to be created at the board level\n",
    "def compute_board_combos(board_name):\n",
    "    board = boards[board_name]\n",
    "    all_possible_combs = list(itertools.combinations(board, 2))\n",
    "    combs_df = pd.DataFrame(all_possible_combs, columns =['Word1', 'Word2'])\n",
    "    combs_df[\"wordpair\"] = combs_df[\"Word1\"] + '-'+ combs_df[\"Word2\"]\n",
    "    return combs_df\n",
    "\n",
    "allcombinations_df = pd.DataFrame(columns=['Board', 'Word1','Word2', 'wordpair'])\n",
    "board_combos = {board_name : compute_board_combos(board_name) for board_name in boards.keys()}\n",
    "for board in board_combos:\n",
    "    newdf = board_combos[board]\n",
    "    newdf.insert(loc=0, column='Board', value=board)\n",
    "    allcombinations_df = pd.concat([allcombinations_df, newdf])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KAGGI8X4cHcR"
   },
   "source": [
    "now that we have the combos, we can make a little helper function to get the wordpair lists for a given board as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 255,
     "status": "ok",
     "timestamp": 1620787533863,
     "user": {
      "displayName": "Robert Hawkins",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiqGTtXNDJINdLVVy9iBBFVLUYe9UyQWhEaBwPLqw=s64",
      "userId": "13623832260192960306"
     },
     "user_tz": 420
    },
    "id": "uHrJuIJNAOud",
    "outputId": "327a8731-ef50-46c1-9e13-5b2e51d9fa3c"
   },
   "outputs": [],
   "source": [
    "def get_wordpair_list(board_combos, board_name) :\n",
    "    return list(board_combos[board_name]['wordpair'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BsZEuT1ddtCz"
   },
   "source": [
    "### get matrix of similarities\n",
    "this serves as our literal semantics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P7uR4l5gAQ4e"
   },
   "outputs": [],
   "source": [
    "## we need to create a exp(clue-w1 * clue-cw2) matrix of size Nx190 for each board\n",
    "## where N is the size of the search space\n",
    "## this is normalized by-row for literal guesser\n",
    "## normalized by column for pragmatic speaker\n",
    "## and then normalized by row again for pragmetic guesser\n",
    "def create_board_matrix(combs_df, context_board, embeddings):\n",
    "    # grab subset of words in given board and their corresponding glove vectors\n",
    "    board_df = sample_df[sample_df['Word'].isin(context_board)]\n",
    "    board_word_indices = list(board_df.index)\n",
    "    board_words = board_df[\"Word\"]\n",
    "    board_vectors = embeddings[board_word_indices]\n",
    "\n",
    "    ## clue_sims is the similarity of ALL clues in full searchspace (size N) to EACH word on board (size 20)\n",
    "    clue_sims = 1 - scipy.spatial.distance.cdist(board_vectors, embeddings, 'cosine')\n",
    "\n",
    "    ## once we have the similarities of the clue to the words on the board\n",
    "    ## we define a multiplicative function that maximizes these similarities\n",
    "    board_df.reset_index(inplace = True)\n",
    "\n",
    "    ## next we find the product of similarities between c-w1 and c-w2 for that specific board's 190 word-pairs\n",
    "    ## this gives us a 190 x N array of product similarities for a given combs_df\n",
    "    ## specifically, for each possible pair, pull out \n",
    "    f_w1_list =  np.array([clue_sims[board_df[board_df[\"Word\"]==row[\"Word1\"]].index.values[0]]\n",
    "                         for  index, row in combs_df.iterrows()])\n",
    "    f_w2_list =  np.array([clue_sims[board_df[board_df[\"Word\"]==row[\"Word2\"]].index.values[0]] \n",
    "                         for  index, row in combs_df.iterrows()])\n",
    "\n",
    "    # result is of length 190 for the product of similarities (i.e. how similar each word i is to BOTH in pair)\n",
    "    # note that cosine is in range [-1, 1] so we have to convert to [0,1] for this conjunction to be valid\n",
    "    return ((f_w1_list + 1) /2) * ((f_w2_list + 1)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q-6S5SOBA3Gn"
   },
   "outputs": [],
   "source": [
    "board_matrices = {\n",
    "    key : {board_name : create_board_matrix(board_combos[board_name], boards[board_name], embedding) \n",
    "           for board_name in boards.keys()}\n",
    "    for (key, embedding) in representations.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xTFBYseDdy2E"
   },
   "source": [
    "### Literal Guesser NP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "given a 190xN matrix of clue-w1 * clue-w2 products, the literal guesser computes softmax over pairs for each possible clue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "skjCu_CJK85n"
   },
   "outputs": [],
   "source": [
    "def literal_guesser_np(board_name, representation):\n",
    "    boardmatrix = board_matrices[representation][board_name]\n",
    "    return softmax(boardmatrix, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UnCh3E62d1TN"
   },
   "source": [
    "### Pragmatic Speaker NP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "given a Nx190 matrix of clue products, pragmatic speaker first computes literal guesser softmax for each clue in searchspace, then computes softmax over all clues for a specific word-pair. this yields a Nx190 array with literal guesser softmax values for each possible clue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "need to implement S1 = beta * (ln(G0)- cost).\n",
    "\n",
    "we use frequency-based cost (higher frequency means lower cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6LELBQn2DIFT"
   },
   "outputs": [],
   "source": [
    "def pragmatic_speaker_np(board_name, beta, costweight, representation):\n",
    "    literal_guesser_prob = np.log(literal_guesser_np(board_name, representation))\n",
    "    clues_cost = -np.array(list(sample_df[\"LgSUBTLWF\"]))\n",
    "    utility = (1-costweight) * literal_guesser_prob - costweight * clues_cost\n",
    "    return softmax(beta * utility, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZieYkjDMd3ws"
   },
   "source": [
    "### Pragmatic Guesser NP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a 190xN matrix of clue products, pragmatic guesser computes pragamtic speaker softmax for EACH wordpair given a particular clue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wrz24elMGEiK"
   },
   "outputs": [],
   "source": [
    "def pragmatic_guesser_np(board_name, beta, costweight, representation):\n",
    "    return softmax(np.log(pragmatic_speaker_np(board_name, beta, costweight, representation)), axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7chhjmljeRzj"
   },
   "source": [
    "### Test models on example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 859,
     "status": "ok",
     "timestamp": 1620782275269,
     "user": {
      "displayName": "Robert Hawkins",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiqGTtXNDJINdLVVy9iBBFVLUYe9UyQWhEaBwPLqw=s64",
      "userId": "13623832260192960306"
     },
     "user_tz": 420
    },
    "id": "5LqRAmn4bHv1",
    "outputId": "fadcb5c2-7fd9-4d93-a3e1-948c0de148b9"
   },
   "outputs": [],
   "source": [
    "clue = 'equation'\n",
    "target = 'exam-algebra'\n",
    "wordpairlist = get_wordpair_list(board_combos, 'e1_board1_words')\n",
    "target_index = wordpairlist.index(target)\n",
    "clue_index = list(sample_df[\"Word\"]).index(clue)\n",
    "\n",
    "a = literal_guesser_np('e1_board1_words', 'glove')[:,clue_index]\n",
    "y = pragmatic_speaker_np('e1_board1_words', 18.858, 0.004, 'glove')\n",
    "top10 = y[target_index,:].argsort()[-5:][::-1].tolist()\n",
    "top10_words = [list(sample_df[\"Word\"])[x] for x in top10]\n",
    "z = pragmatic_guesser_np('e1_board1_words', 18.858, 0.004, 'glove')[:,clue_index]\n",
    "\n",
    "print(\"literal guesser prediction is:\", wordpairlist[np.argmax(a)])\n",
    "print(\"top10 prag speaker predictions are:\", top10_words)\n",
    "print(\"pragmatic guesser prediction is:\", wordpairlist[np.argmax(z)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IBbehImBsZVn"
   },
   "source": [
    "# Generate RSA predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 305,
     "status": "ok",
     "timestamp": 1620785211948,
     "user": {
      "displayName": "Robert Hawkins",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiqGTtXNDJINdLVVy9iBBFVLUYe9UyQWhEaBwPLqw=s64",
      "userId": "13623832260192960306"
     },
     "user_tz": 420
    },
    "id": "ApJNxZzktNIE",
    "outputId": "cd767e1c-eb5b-4b93-d596-9a885674df39"
   },
   "outputs": [],
   "source": [
    "## create boards and merge with expdata\n",
    "combined_boards_df = pd.DataFrame(columns=['Experiment', 'Board','boardwords'])\n",
    "combined_boards_df[\"Experiment\"]  = [\"E1\"] * 10 + [\"E2\"] * 10\n",
    "combined_boards_df[\"Board\"] = [\"TrialList\" + str(i) for i in range(1,11)] * 2\n",
    "combined_boards_df[\"boardnames\"] = (['e1_board' + str(i) + '_words' for i in range(1,11)] \n",
    "                                  + ['e2_board' + str(i) + '_words' for i in range(1,11)])\n",
    "combined_boards_df[\"boardwords\"] = [boards[n] for n in combined_boards_df[\"boardnames\"]]\n",
    "combined_boards_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 331,
     "status": "ok",
     "timestamp": 1620785461063,
     "user": {
      "displayName": "Robert Hawkins",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiqGTtXNDJINdLVVy9iBBFVLUYe9UyQWhEaBwPLqw=s64",
      "userId": "13623832260192960306"
     },
     "user_tz": 420
    },
    "id": "k0ax0K8mVL04",
    "outputId": "e98f4d92-6f0a-4a1b-92db-2e9f382534f9"
   },
   "outputs": [],
   "source": [
    "## need to get similarity matrix of these words in this order to work with\n",
    "target_df = pd.read_csv(\"../data/connector_wordpairs_boards.csv\")\n",
    "target_df[\"wordpair\"]= target_df[\"Word1\"]+ \"-\"+target_df[\"Word2\"]\n",
    "target_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5MoszZoUXKTU"
   },
   "source": [
    "## Speaker predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the best-fitting params for each model (see Optimizing parameters section below for code used to find these values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7o81fCYkj6Yl"
   },
   "outputs": [],
   "source": [
    "rsa_optimal_params = {\n",
    "    'swow' : (25.1522030761838, 0.03863169001849234),\n",
    "    'glove' : (22.336514544537227, 0.039),\n",
    "    'bert-sum' : (29.709602301411962, 0.031659060110267576), #-17533\n",
    "}\n",
    "\n",
    "board_optimal_params = {\n",
    "    'swow' : (23.488850322875496, 1), # -13204\n",
    "    'glove' : (20.952928531665275, 1), # -15774.814774380024)\n",
    "    'bert-sum' : (19.983835225540847, 0.787924454045298),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute scores/ranks\n",
    "Our main DV is to likelihood of the data, so we compute scores & ranks for all possible clues produced by the participant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_speaker_scores(group, speaker_word_pairs, y, y_sorted) :\n",
    "    speaker_prob = []\n",
    "    speaker_rank = []\n",
    "    for index, row in group.iterrows():\n",
    "        clue1 = row[\"Clue1\"]\n",
    "        wordpair = str(row[\"wordpair\"]).replace(\" \", \"\")\n",
    "        wordpair_index = speaker_word_pairs.index(wordpair)\n",
    "        w1_index, w2_index = [list(sample_df[\"Word\"]).index(word) for word in wordpair.split('-')]\n",
    "        \n",
    "        # find index of clue\n",
    "        if clue1 in list(sample_df[\"Word\"]):\n",
    "            clue_index = list(sample_df[\"Word\"]).index(clue1)\n",
    "            clue_probs = y[wordpair_index, clue_index]\n",
    "            clue_rank = np.nonzero(y_sorted==clue_index)[1][wordpair_index]\n",
    "        else:\n",
    "            clue_rank = \"NA\"\n",
    "            clue_probs = \"NA\"\n",
    "\n",
    "        speaker_prob.append(clue_probs)\n",
    "        speaker_rank.append(clue_rank)\n",
    "    return speaker_prob, speaker_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 42011,
     "status": "ok",
     "timestamp": 1620796384376,
     "user": {
      "displayName": "Robert Hawkins",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiqGTtXNDJINdLVVy9iBBFVLUYe9UyQWhEaBwPLqw=s64",
      "userId": "13623832260192960306"
     },
     "user_tz": 420
    },
    "id": "diTU6Zk20fZf",
    "outputId": "a50381e4-2a34-448c-87f2-23145c0ab88e"
   },
   "outputs": [],
   "source": [
    "speakerprobs_df = pd.DataFrame(columns=['representation', 'Experiment','Board', \"Word1\", \"Word2\", \"Clue1\", \"clueCount\", \"wordpair\", \"prag_speaker_probs\"])\n",
    "for representation in representations.keys() :\n",
    "    for index, row in combined_boards_df.iterrows():\n",
    "        board = row[\"boardwords\"]\n",
    "        boardname = row[\"boardnames\"]\n",
    "        wordpairlist = get_wordpair_list(board_combos, boardname)\n",
    "        speaker_word_pairs = target_df[(target_df[\"boardnames\"] == row[\"boardnames\"]) & \n",
    "                                       (target_df[\"Experiment\"] == row[\"Experiment\"])][\"wordpair\"]\n",
    "        speaker_word_pairs = list(speaker_word_pairs)\n",
    "        speaker_df_new = pd.DataFrame({'wordpair': speaker_word_pairs})\n",
    "        params = rsa_optimal_params[representation]\n",
    "        speaker_model = pragmatic_speaker_np(boardname, params[0], params[1], representation)\n",
    "\n",
    "        ## this is created at the BOARD level\n",
    "        y = np.array([speaker_model[wordpairlist.index(wordpair)] for wordpair in speaker_word_pairs])\n",
    "        y_sorted = np.argsort(-y)\n",
    "\n",
    "        ## so y has 3 vectors of clue probabilities (the 3 pairs on this board)\n",
    "        ## now we need to go into expdata and score the probabilities for those specific clues\n",
    "        expdata_board = expdata[(expdata[\"Board\"] == row[\"Board\"]) & (expdata[\"Experiment\"] == row[\"Experiment\"])]\n",
    "        speaker_prob, speaker_rank = get_speaker_scores(expdata_board, speaker_word_pairs, y, y_sorted)\n",
    "        expdata_board.loc[:,\"representation\"] = representation\n",
    "        expdata_board.loc[:,\"prag_speaker_probs\"] = speaker_prob\n",
    "        expdata_board.loc[:,\"prag_speaker_rank\"] = speaker_rank\n",
    "        speakerprobs_df = pd.concat([speakerprobs_df, expdata_board])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ESx0Uvm820s8"
   },
   "outputs": [],
   "source": [
    "speakerprobs_df.to_csv(\"../data/speaker_ranks.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute top-n lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "executionInfo": {
     "elapsed": 17341,
     "status": "ok",
     "timestamp": 1620540149699,
     "user": {
      "displayName": "Robert Hawkins",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiqGTtXNDJINdLVVy9iBBFVLUYe9UyQWhEaBwPLqw=s64",
      "userId": "13623832260192960306"
     },
     "user_tz": 240
    },
    "id": "hxld3QX9s3zl",
    "outputId": "6c70c778-f751-4d65-8cfb-fdcdc521bf88"
   },
   "outputs": [],
   "source": [
    "## Creating a mainlist of boards\n",
    "speaker_df = pd.DataFrame(columns=['representation', 'wordpair','Board', 'prag_speaker_words'])\n",
    "\n",
    "for representation in representations.keys() :\n",
    "  for index, row in combined_boards_df.iterrows():\n",
    "    board_words = row[\"boardwords\"]\n",
    "    boardname = row[\"boardnames\"]\n",
    "    wordpairlist = get_wordpair_list(board_combos, boardname)\n",
    "    params = rsa_optimal_params[representation]\n",
    "    speaker_model = pragmatic_speaker_np(boardname, params[0], params[1], representation)\n",
    "    ## get empirical speaker probs for this board\n",
    "    speaker_word_pairs = target_df[(target_df[\"boardnames\"] == row['boardnames']) & \n",
    "                                          (target_df[\"Experiment\"] == row['Experiment'])][\"wordpair\"]\n",
    "    speaker_df_new = pd.DataFrame({'wordpair': speaker_word_pairs})\n",
    "    speaker_df_new[\"Board\"] = boardname\n",
    "    speaker_df_new[\"representation\"] = representation\n",
    "    y = np.array([speaker_model[wordpairlist.index(wordpair)] for wordpair in speaker_word_pairs])\n",
    "\n",
    "    ## this will yield 3x10191 array [1-d array for each word-pair, 3 word-pairs per board]\n",
    "    ## from here we sort each row in descending order \n",
    "    y_sorted = np.argsort(-y) ## gives sorted indices\n",
    "    top20_indices = y_sorted[:,:20]\n",
    "\n",
    "    ## convert to words\n",
    "    w1 = [list(sample_df[\"Word\"])[i] for i in top20_indices[0]]\n",
    "    w2 = [list(sample_df[\"Word\"])[i] for i in top20_indices[1]]\n",
    "    w3 = [list(sample_df[\"Word\"])[i] for i in top20_indices[2]]\n",
    "    words = [w1, w2, w3]\n",
    "    speaker_df_new[\"prag_speaker_words\"] = words\n",
    "    speaker_df = pd.concat([speaker_df, speaker_df_new])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e1R2bUvXLXd5"
   },
   "outputs": [],
   "source": [
    "speaker_df.to_csv(\"../data/speaker_top.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Wl5z7x0sSHr"
   },
   "source": [
    "## Listener predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 84817,
     "status": "ok",
     "timestamp": 1620789421041,
     "user": {
      "displayName": "Robert Hawkins",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiqGTtXNDJINdLVVy9iBBFVLUYe9UyQWhEaBwPLqw=s64",
      "userId": "13623832260192960306"
     },
     "user_tz": 420
    },
    "id": "Pc1Ron6soZiC",
    "outputId": "517a4d89-3523-45ab-9dfb-09ae0c4f6a28"
   },
   "outputs": [],
   "source": [
    "guesser_df = pd.DataFrame(columns=['Experiment','Board', \"Word1\", \"Word2\", \"Clue1\", \"clueCount\", \"wordpair\"])\n",
    "for representation in representations.keys() :\n",
    "    for index, row in combined_boards_df.iterrows():\n",
    "        guesser_df_board = pd.DataFrame(columns=['Experiment','Board', \"Word1\", \"Word2\", \"Clue1\", \"clueCount\", \"wordpair\"])\n",
    "        board = row[\"boardwords\"]\n",
    "        boardname = row['boardnames']\n",
    "        wordpairlist = get_wordpair_list(board_combos, boardname)\n",
    "        # calculate the prag guesser for this specific wordpairlist (corresponding to ONE board)\n",
    "        params = rsa_optimal_params[representation]\n",
    "        x = literal_guesser_np(boardname, representation)\n",
    "        z = pragmatic_guesser_np(boardname, params[0], params[1], representation) \n",
    "\n",
    "        # then loop through the clues in expdata_board to get predictions\n",
    "        expdata_board = expdata[(expdata[\"Board\"] == row[\"Board\"]) & (expdata[\"Experiment\"] == row[\"Experiment\"])]\n",
    "        expdata_board.loc[:, \"representation\"] = representation\n",
    "\n",
    "        for index, row in expdata_board.iterrows():\n",
    "            clue1 = row[\"Clue1\"]\n",
    "            if clue1 in list(sample_df[\"Word\"]):\n",
    "                ## literal guesser uses \"x\", pragmatic guesser uses \"z\"\n",
    "                clue_index = list(sample_df[\"Word\"]).index(clue1)\n",
    "                literal_pred = wordpairlist[np.argmax(x[:,clue_index])]\n",
    "                pragmatic_pred = wordpairlist[np.argmax(z[:,clue_index])]\n",
    "            else:\n",
    "                literal_pred = \"NA\"\n",
    "                pragmatic_pred = \"NA\"\n",
    "\n",
    "            # we want to track likelihood for ALL responses (i.e. full listener distribution)\n",
    "            guesser_df_clue = pd.DataFrame({\n",
    "                'Clue1' : clue1, \n",
    "                'possible_wordpair' : wordpairlist,\n",
    "                'literal_likelihood' : x[:, clue_index],\n",
    "                'prag_likelihood' : z[:, clue_index],\n",
    "                'literal_top_prediction' : literal_pred,\n",
    "                'prag_top_prediction' : pragmatic_pred\n",
    "            })\n",
    "            guesser_df_board = pd.concat([guesser_df_board, pd.merge(expdata_board, guesser_df_clue)])\n",
    "        guesser_df = pd.concat([guesser_df, guesser_df_board])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0CU0nWdnpCGt"
   },
   "outputs": [],
   "source": [
    "guesser_df.to_csv(\"../data/guesser_scores.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate 'naive' context model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SEzXZ7_r5I9B"
   },
   "outputs": [],
   "source": [
    "## define a func that computed similarities of clue to each word on the board\n",
    "## and then maximizes similarity to the words while minimizing similarity to other words\n",
    "def speaker_board_func(combs_df, context_board, alpha, beta, representation_model):\n",
    "    # grab subset of words in given board and their corresponding glove vectors\n",
    "    board_df = sample_df[sample_df['Word'].isin(context_board)]\n",
    "    board_word_indices = list(board_df.index)\n",
    "    board_words = board_df[\"Word\"]\n",
    "    board_vectors = representation_model[board_word_indices]\n",
    "\n",
    "    ## clue_sims is the similarity of ALL clues in full searchspace (size N) to EACH word on board (size 20)\n",
    "    clue_sims = (1-scipy.spatial.distance.cdist(board_vectors, representation_model, 'cosine') + 1 ) / 2\n",
    "    target_sample = target_df[target_df['Word1'].isin(board_df[\"Word\"]) & target_df['Word2'].isin(board_df[\"Word\"])]\n",
    "    w1_index = [list(board_df[\"Word\"]).index(row[\"Word1\"]) for index, row in target_sample.iterrows()]\n",
    "    w2_index = [list(board_df[\"Word\"]).index(row[\"Word2\"]) for index, row in target_sample.iterrows()]\n",
    "    clue_w1 = clue_sims[w1_index]\n",
    "    clue_w2 = clue_sims[w2_index]\n",
    "    clue_prod = np.multiply(clue_w1,clue_w2)\n",
    "\n",
    "    # deleting the two target words to compute average similarity to other words on the board\n",
    "    clue_sims_new = np.array([np.delete(clue_sims, [w1_index[i], w2_index[i]], axis=0) for i in range(len(w1_index))])\n",
    "    avg_sim = np.mean(clue_sims_new, axis=1)\n",
    "\n",
    "    ## FUNC = alpha(clue_w1*clue_w2) + (1-alpha)*(average of other board words)\n",
    "\n",
    "    func = np.subtract((alpha)*clue_prod, (1-alpha)*avg_sim)\n",
    "    return softmax(beta * func, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 265,
     "status": "ok",
     "timestamp": 1620751556240,
     "user": {
      "displayName": "Abhilasha Kumar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh3S_kn7pEo0LnvQDjGbOGbx0BmsP7fZY-0waKLnQ=s64",
      "userId": "00864468883555656933"
     },
     "user_tz": 300
    },
    "id": "CfYcPZiG13dd",
    "outputId": "11066401-87d3-4dc1-9297-02e13b11df92"
   },
   "outputs": [],
   "source": [
    "check_sum = np.sum(speaker_board_func(board_combos['e1_board10_words'], boards['e1_board10_words'], 0.7, 20, representations['glove']), axis = 1) \n",
    "assert all(check_sum == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3NagfI7CbmqM"
   },
   "source": [
    "## Speaker predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "executionInfo": {
     "elapsed": 52847,
     "status": "ok",
     "timestamp": 1620679544331,
     "user": {
      "displayName": "Abhilasha Kumar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh3S_kn7pEo0LnvQDjGbOGbx0BmsP7fZY-0waKLnQ=s64",
      "userId": "00864468883555656933"
     },
     "user_tz": 300
    },
    "id": "I1uF2pvP-NF2",
    "outputId": "84dcfba8-3d7c-49d4-b609-7fc6e8296ad2"
   },
   "outputs": [],
   "source": [
    "speaker_board_df = pd.DataFrame(columns=['Word1', 'Word2', 'boardnames','top10preds', 'alpha', 'Model'])\n",
    "for representation in ['bert-sum', 'glove', 'swow']: \n",
    "    for alpha in np.arange(0,1.1, 0.1):\n",
    "        ## for a given alpha, compute the clue similarities \n",
    "        params = board_optimal_params[representation]\n",
    "        speaker_board_probs = {\n",
    "            board_name : speaker_board_func(board_combos[board_name], boards[board_name], alpha, params[0], representations[representation]) \n",
    "            for board_name in boards.keys()\n",
    "        }   \n",
    "        # we calculate the top5 speaker predictions for each word-pair based on highest value above\n",
    "        for board in speaker_board_probs.keys():\n",
    "            ## obtain top10 indices for each word-pair\n",
    "            idx = [(-speaker_board_probs[board][x]).argsort()[:10].tolist() for x in range(3)]\n",
    "            a = [list(sample_df[\"Word\"])[z] for y in idx for z in y]\n",
    "            top10preds = [list(arr) for arr in np.array_split(a, 3)]\n",
    "            speaker_df_new = pd.DataFrame({'boardnames': [board]*3})\n",
    "            speaker_df_new[\"Word1\"] = list(target_df[target_df['boardnames']== board][\"Word1\"])\n",
    "            speaker_df_new[\"Word2\"] = list(target_df[target_df['boardnames']== board][\"Word2\"])\n",
    "            speaker_df_new[\"alpha\"] = [alpha]*3\n",
    "            speaker_df_new[\"top10preds\"] = top10preds\n",
    "            speaker_df_new[\"Model\"] = representation\n",
    "            speaker_board_df = pd.concat([speaker_board_df, speaker_df_new])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wGfbQB-b-VuL"
   },
   "outputs": [],
   "source": [
    "speaker_board_df.to_csv(\"../data/speaker_boardfunc_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FlMlVQRIGK_j"
   },
   "source": [
    "## Obtain Clue Score for every possible Clue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 276,
     "status": "ok",
     "timestamp": 1620793902197,
     "user": {
      "displayName": "Robert Hawkins",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiqGTtXNDJINdLVVy9iBBFVLUYe9UyQWhEaBwPLqw=s64",
      "userId": "13623832260192960306"
     },
     "user_tz": 420
    },
    "id": "XkB5vViPKXDo",
    "outputId": "28b7f69a-714c-4204-d1e9-132c26c0d262"
   },
   "outputs": [],
   "source": [
    "## merge expdata with combined_boards so we have the \"boardname\" correct\n",
    "expdata_new = pd.merge(expdata,combined_boards_df,on=['Board', 'Experiment'],how='left')\n",
    "expdata_new[\"wordpair\"] = expdata_new[\"Word1\"] + \"-\" + expdata_new[\"Word2\"]\n",
    "expdata_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 348,
     "status": "ok",
     "timestamp": 1620793859201,
     "user": {
      "displayName": "Robert Hawkins",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiqGTtXNDJINdLVVy9iBBFVLUYe9UyQWhEaBwPLqw=s64",
      "userId": "13623832260192960306"
     },
     "user_tz": 420
    },
    "id": "FkLwu5mHMH-o",
    "outputId": "1fb839ac-f9b6-4c65-8421-a69947a4b112"
   },
   "outputs": [],
   "source": [
    "target_df[\"wordpair\"] = target_df[\"Word1\"] + \"-\" + target_df[\"Word2\"]\n",
    "target_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "executionInfo": {
     "elapsed": 611572,
     "status": "ok",
     "timestamp": 1620794540290,
     "user": {
      "displayName": "Robert Hawkins",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiqGTtXNDJINdLVVy9iBBFVLUYe9UyQWhEaBwPLqw=s64",
      "userId": "13623832260192960306"
     },
     "user_tz": 420
    },
    "id": "F5nrF5MCILaH",
    "outputId": "f6080071-848f-4869-b8cf-669e63eb69d5"
   },
   "outputs": [],
   "source": [
    "clue_board_df_main = pd.DataFrame(columns=['boardnames','wordpair', 'Clue1', 'clue_score', 'alpha', 'Model'])\n",
    "\n",
    "for representation in ['bert-sum', 'glove', 'swow']: \n",
    "  for alpha in np.arange(0,1.1, 0.1):\n",
    "    ## for a given alpha, compute the clue similarities \n",
    "    beta = board_optimal_params[representation][0]\n",
    "    speaker_board_probs = {\n",
    "        board_name : speaker_board_func(board_combos[board_name], boards[board_name], alpha, beta, representations[representation]) \n",
    "        for board_name in boards.keys()\n",
    "    }   \n",
    "    \n",
    "    for board in speaker_board_probs.keys():\n",
    "      \n",
    "      ## get the clues we need scores for from expdatanew\n",
    "      clue_main = expdata_new.loc[expdata_new['boardnames'] == board]\n",
    "      target_main = target_df.loc[target_df['boardnames'] == board]\n",
    "      \n",
    "      target_main.reset_index(inplace = True)\n",
    "      #print(target_main)\n",
    "\n",
    "      for index, row in clue_main.iterrows():\n",
    "        if row[\"Clue1\"] in list(sample_df[\"Word\"]):\n",
    "          #print(\"clue is:\", row[\"Clue1\"])\n",
    "          clue_index = list(sample_df[\"Word\"]).index(row[\"Clue1\"])\n",
    "          #print(\"clue_index:\",clue_index)\n",
    "          wordpair = row[\"wordpair\"]\n",
    "          ## need to figure out specific wordpair this clue corresponds to\n",
    "          wordpair_index = target_main.index[(target_main['wordpair'] == wordpair)].tolist()[0]\n",
    "          #print(\"wordpair_index:\",wordpair_index)\n",
    "          # get a sorted array of the clue scores\n",
    "          mainscores = speaker_board_probs[board][wordpair_index]\n",
    "          sorted_clue_probs = np.argsort(-mainscores).tolist()\n",
    "          #print(\"sorted_clue_probs_indices = \", sorted_clue_probs)\n",
    "          \n",
    "          # we next obtain the score for each clue for a specific wordpair\n",
    "          clue_similarity = speaker_board_probs[board][wordpair_index][clue_index]\n",
    "          # want to find index of this particular clue in the overall distribution\n",
    "          clue_rank = sorted_clue_probs.index(clue_index)\n",
    "          #print(\"clue_rank:\",clue_rank)\n",
    "        else:\n",
    "          clue_similarity = \"NA\"\n",
    "          clue_rank = \"NA\"\n",
    "        \n",
    "        clue_board_df = pd.DataFrame({'boardnames': [board]})\n",
    "        clue_board_df[\"wordpair\"] = wordpair\n",
    "        clue_board_df[\"Clue1\"] = row[\"Clue1\"]\n",
    "        clue_board_df[\"clue_score\"] = clue_similarity\n",
    "        clue_board_df[\"clue_rank\"] = clue_rank\n",
    "        clue_board_df[\"alpha\"] = alpha\n",
    "        clue_board_df[\"Model\"] = representation\n",
    "          \n",
    "        clue_board_df_main = pd.concat([clue_board_df_main, clue_board_df])\n",
    "\n",
    "clue_board_df_main                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jDWHqi99Lm_8"
   },
   "outputs": [],
   "source": [
    "clue_board_df_main.to_csv(\"../data/speaker_boardfunc_df_ranks_softmax.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HvfW-6IsLt-r"
   },
   "source": [
    "# Optimizing model params\n",
    "\n",
    "Our speaker models have two free parameters. In order to make a fair comparison across different representations, we want to find best version of each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lMgt8GxNW-nr"
   },
   "outputs": [],
   "source": [
    "softplus = lambda x: np.log1p(np.exp(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize 'board' models (i.e. non-RSA way of incorporating context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 783
    },
    "executionInfo": {
     "elapsed": 121930,
     "status": "error",
     "timestamp": 1620788884098,
     "user": {
      "displayName": "Robert Hawkins",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiqGTtXNDJINdLVVy9iBBFVLUYe9UyQWhEaBwPLqw=s64",
      "userId": "13623832260192960306"
     },
     "user_tz": 420
    },
    "id": "oO0LzmeoRH0y",
    "outputId": "ebf5f8b2-c4f6-4929-9d6f-8cbf0f69cd20"
   },
   "outputs": [],
   "source": [
    "representation = 'glove'\n",
    "def get_board_speaker_likelihood(params):\n",
    "    speaker_prob = []\n",
    "    beta = softplus(params[0])\n",
    "    alpha = expit(params[1])\n",
    "    representation_model = representations[representation]\n",
    "    for index, row in combined_boards_df.iterrows():\n",
    "        # grab subset of words in given board and their corresponding glove vectors\n",
    "        boardname = row[\"boardnames\"]\n",
    "        board_df = sample_df[sample_df['Word'].isin(boards[boardname])]\n",
    "        board_word_indices = list(board_df.index)\n",
    "        board_words = board_df[\"Word\"]\n",
    "        board_vectors = representation_model[board_word_indices]\n",
    "\n",
    "        ## clue_sims is the similarity of ALL clues in full searchspace (size N) to EACH word on board (size 20)\n",
    "        clue_sims = (1-scipy.spatial.distance.cdist(board_vectors, representation_model, 'cosine') + 1) / 2\n",
    "        target_sample = target_df[target_df['Word1'].isin(board_df[\"Word\"]) & target_df['Word2'].isin(board_df[\"Word\"])]\n",
    "        w1_index = [list(board_df[\"Word\"]).index(row[\"Word1\"]) for index, row in target_sample.iterrows()]\n",
    "        w2_index = [list(board_df[\"Word\"]).index(row[\"Word2\"]) for index, row in target_sample.iterrows()]\n",
    "        clue_w1 = clue_sims[w1_index]\n",
    "        clue_w2 = clue_sims[w2_index]\n",
    "        clue_prod = np.multiply(clue_w1, clue_w2)\n",
    "        clue_sims_new = np.array([np.delete(clue_sims, [w1_index[i], w2_index[i]], axis=0) for i in range(len(w1_index))])\n",
    "        avg_sim = np.mean(clue_sims_new, axis=1)\n",
    "        func = np.subtract((alpha)*clue_prod, (1-alpha)*avg_sim)\n",
    "        y = softmax(beta * func, axis=1)\n",
    "        expdata_board = expdata[(expdata[\"Board\"] == row[\"Board\"]) & (expdata[\"Experiment\"] == row[\"Experiment\"])]\n",
    "        speaker_word_pairs = list(target_sample['wordpair'])\n",
    "        for index, row in expdata_board.iterrows():\n",
    "            wordpair = str(row[\"wordpair\"]).replace(\" \", \"\")\n",
    "            wordpair_index = speaker_word_pairs.index(wordpair)\n",
    "            clue1 = row[\"Clue1\"]\n",
    "            if clue1 in list(sample_df[\"Word\"]):\n",
    "                # find index of clue\n",
    "                clue_index = list(sample_df[\"Word\"]).index(clue1)\n",
    "                clue_probs = y[wordpair_index, clue_index]\n",
    "                speaker_prob.append(row['clueCount'] * np.log(clue_probs))\n",
    "    print(beta, alpha, '(', params[1], ')', ':', np.sum(speaker_prob))\n",
    "    return -np.sum(speaker_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these lines gives a good 'initialization' for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scipy.optimize.minimize(get_board_speaker_likelihood, [24, 3.349]) # optimize for 'swow'\n",
    "#scipy.optimize.minimize(get_board_speaker_likelihood, [20.6, 3.349]) # optimize for 'glove'\n",
    "scipy.optimize.minimize(get_board_speaker_likelihood, [1, 5]) # optimize for 'bert-sum'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize RSA speaker models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 902
    },
    "executionInfo": {
     "elapsed": 276789,
     "status": "error",
     "timestamp": 1620788608520,
     "user": {
      "displayName": "Robert Hawkins",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiqGTtXNDJINdLVVy9iBBFVLUYe9UyQWhEaBwPLqw=s64",
      "userId": "13623832260192960306"
     },
     "user_tz": 420
    },
    "id": "PRtJFUjBzbJe",
    "outputId": "8c27cd24-ee09-42bc-f98e-a7b9550b85ae"
   },
   "outputs": [],
   "source": [
    "representation = 'swow'\n",
    "\n",
    "def get_rsa_speaker_likelihood(params) :\n",
    "  # params is a list\n",
    "  speaker_prob = []\n",
    "  beta = softplus(params[0])\n",
    "  costweight = expit(params[1])\n",
    "  for index, row in combined_boards_df.iterrows():\n",
    "    board = row[\"boardwords\"]\n",
    "    boardname = row[\"boardnames\"]\n",
    "    wordpairlist = get_wordpair_list(board_combos, boardname)\n",
    "    speaker_word_pairs = target_df[(target_df[\"boardnames\"] == row[\"boardnames\"]) & \n",
    "                                   (target_df[\"Experiment\"] == row[\"Experiment\"])][\"wordpair\"]\n",
    "    speaker_model = pragmatic_speaker_np(boardname, beta, costweight, representation)\n",
    "    y = np.array([speaker_model[wordpairlist.index(wordpair)] for wordpair in speaker_word_pairs])\n",
    "    expdata_board = expdata[(expdata[\"Board\"] == row[\"Board\"]) & (expdata[\"Experiment\"] == row[\"Experiment\"])]\n",
    "    speaker_word_pairs = list(speaker_word_pairs)\n",
    "    for index, row in expdata_board.iterrows():\n",
    "      wordpair = str(row[\"wordpair\"]).replace(\" \", \"\")\n",
    "      wordpair_index = speaker_word_pairs.index(wordpair)\n",
    "      clue1 = row[\"Clue1\"]\n",
    "      if clue1 in list(sample_df[\"Word\"]):\n",
    "        clue_index = list(sample_df[\"Word\"]).index(clue1)\n",
    "        clue_probs = y[wordpair_index, clue_index]\n",
    "        speaker_prob.append(row['clueCount'] * np.log(clue_probs))\n",
    "  print(beta, costweight, '(', params[1], ')', ':', np.sum(speaker_prob))\n",
    "  return -np.sum(speaker_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scipy.optimize.minimize(get_board_speaker_likelihood, [24, 3.349]) # optimize for 'swow'\n",
    "#scipy.optimize.minimize(get_board_speaker_likelihood, [20.6, 3.349]) # optimize for 'glove'\n",
    "scipy.optimize.minimize(get_rsa_speaker_likelihood, [25.399, -3.219]) # optimize for 'swow'"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
