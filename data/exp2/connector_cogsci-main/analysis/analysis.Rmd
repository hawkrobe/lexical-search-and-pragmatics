---
title: "Connector-RSA"
output: pdf_document
---

# Imports

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidyboot)
library(here)
library(ggthemes)
library(GGally)
library(lme4)
library(lmerTest)
```

clean up empirical data from the connector games.

```{r}
c <- read_csv(here("./data/raw_data.csv")) %>%
  filter(!is.na(P2W1)) %>%
  unite(wordpair, Word1, Word2, sep = '-') %>%
  unite(chosen_wordpair1, P2W1, P2W2, sep = '-', remove = F) %>%
  unite(chosen_wordpair2, P2W2, P2W1, sep = '-', remove = T) %>%
  select(Subject, Experiment, wordpair, Trial, Clue1, chosen_wordpair1, chosen_wordpair2) %>%
  rename(true_wordpair = wordpair)

c_clues = read_csv(here("./data/final_board_clues_all.csv")) %>% 
  mutate(wordpair = gsub(' - ', '-', wordpair)) %>%
  distinct()
```

# Behavioral results

We import the csv generated by the `descriptive_analyses.ipynb` notebook.

```{r}
c.sims = read_csv(here("../data/descriptive_precomputed.csv")) %>%
  unite("wordpair", Word1, Word2, sep = '-') %>%
  pivot_longer(names_to = 'comparison', values_to = 'sim', cols = `bert-sum_w1w2_sim`:`swow_c2_c1avg_sim`) %>%
  separate(comparison, sep='_', into = c('representation', 'comparison'), extra = 'merge') %>%
  mutate(representation = fct_recode(representation, BERT = "bert-sum", GloVe = "glove", SWOW = "swow"),
         representation = fct_relevel(representation, "GloVe", "BERT", "SWOW")) 
```

## Fig.2: Clue 1 

```{r}

## plot of clue 1 to midpoint and w1w2

c.sims %>%
  group_by(Subject, wordpair, representation, comparison) %>%
  pivot_wider(names_from = comparison, values_from = sim) %>%
  group_by(representation, wordpair, Subject) %>%
  transmute(`farther\nword` = min(c1w1_sim, c1w2_sim),
            `closer\nword` = max(c1w1_sim, c1w2_sim),
            midpoint = c1avg_sim) %>%
  pivot_longer(cols = -c(Subject, wordpair, representation), names_to = 'comparison') %>%
  Rmisc::summarySE(data = .,
                   measurevar = "value", groupvars = c("representation", "comparison"),
                   na.rm = TRUE) %>%
  mutate(comparison = fct_relevel(comparison, "farther\nword", "closer\nword", "midpoint"),
         type = "first clue to target words and midpoint",
           representation = fct_relevel(representation, "BERT", "SWOW", "GloVe")) %>%
  ggplot(aes(x = comparison, y = value, color = representation, group = representation)) +
        geom_errorbar(aes(ymin=value - ci, ymax=value + ci), width=0.1, size = 1,
                  color = "gray26", position = position_dodge(0))+
    geom_point(size = 2) + 
    geom_line(size = 1)+
    scale_color_solarized() +
    labs(x = "", y = "cosine similarity") +
  ggtitle("first clue to target words\nand midpoint")+
    theme_few()+
      ylim(0.1, 0.8) +
    theme(aspect.ratio = 1)+
  theme( axis.text.x = element_text( size = rel(2)),
         axis.text.y = element_text(size = rel(2)),
         strip.text.y = element_text(face = "bold", size = rel(1.3)),
     axis.title = element_text( size = rel(2)),
          legend.title = element_text( size = rel(2)),
     legend.text = element_text( size = rel(1.8)),
         plot.title = element_text(hjust = .5, size = rel(1.8)),
         strip.text.x = element_text(face = "bold", size = rel(1)))

## plot of clue1 to midpoint across easy/med/hard

c.sims %>%
  group_by(Subject, Level, wordpair, representation, comparison) %>%
  pivot_wider(names_from = comparison, values_from = sim) %>%
  group_by(representation, Level, wordpair, Subject) %>%
  transmute(midpoint = c1avg_sim) %>%
  pivot_longer(cols = -c(Subject, Level, wordpair, representation), names_to = 'comparison') %>%
  Rmisc::summarySE(data = .,
                   measurevar = "value", groupvars = c("representation", "Level", "comparison"),
                   na.rm = TRUE) %>%
  mutate(comparison = fct_relevel(Level, "Easy", "Medium", "Hard")) %>%
  select(-Level)%>%
  mutate(comparison = fct_recode(comparison, 
  `close\npair` = "Easy", `medium\npair` = "Medium", `distant\npair` =  "Hard"),
  type = "first clue to midpoint across target pair distance",
  representation = fct_relevel(representation, "BERT", "SWOW", "GloVe")) %>%
  ggplot(aes(x = comparison, y = value, color = representation, group = representation)) + 
          geom_errorbar(aes(ymin=value - ci, ymax=value + ci), width=0.1, size = 1,
                  color = "gray26", position = position_dodge(0))+
    geom_point(size = 2) + 
    geom_line(size = 1)+
    scale_color_solarized() +
    labs(x = "", y = "cosine similarity") +
    theme_few()+
        ylim(0.1, 0.8) +
    theme(aspect.ratio = 1)+
  ggtitle("first clue to midpoint across\ntarget pair distance")+
  theme( axis.text.x = element_text( size = rel(2)),
         axis.text.y = element_text(size = rel(2)),
         strip.text.y = element_text(face = "bold", size = rel(1.3)),
     axis.title = element_text( size = rel(2)),
          legend.title = element_text( size = rel(2)),
     legend.text = element_text( size = rel(1.8)),
         plot.title = element_text(hjust = .5, size = rel(1.8)),
         strip.text.x = element_text(face = "bold", size = rel(1)))



```

## Clue 1 Stats

```{r}
## ranef of whether first clue is closer to midpoint/w1/w2

clue1_ranef_data = c.sims %>%
  group_by(Subject, Level, wordpair, representation, comparison) %>%
  pivot_wider(names_from = comparison, values_from = sim) %>%
  group_by(representation, wordpair, Subject) %>%
  transmute(c1w1 = min(c1w1_sim, c1w2_sim),
            c1w2 = max(c1w1_sim, c1w2_sim),
            midpoint = c1avg_sim) %>%
  pivot_longer(cols = -c(Subject, wordpair, representation), names_to = 'comparison') %>%
  nest(-representation) %>% 
  mutate(results = map(data, ~ {
     lmer(value ~ comparison + (1 | Subject) + (1|wordpair), 
          contrasts = list(comparison = contr.treatment(3, base = 2)),
          data = .) %>% 
      broom.mixed::tidy()
    })) %>%
  unnest(results) %>%
  filter(term %in% c('comparison1', 'comparison3'))
  

## ranef of clue1 proximity to midpoint as a function of difficulty

 c.sims %>%
  group_by(Subject, Level, wordpair, representation, comparison) %>%
  pivot_wider(names_from = comparison, values_from = sim) %>%
  group_by(representation, wordpair, Subject, Level) %>%
  transmute(midpoint = c1avg_sim) %>%
  pivot_longer(cols = -c(Subject, Level, wordpair, representation), names_to = 'comparison') %>%
  nest(-representation) %>% 
  mutate(results = map(data, ~ {
     lmer(value ~ Level + (1 | Subject) + (1|wordpair), 
          contrasts = list(Level = contr.treatment(3, base = 2)),
          data = .) %>% 
      broom.mixed::tidy()
    })) %>%
  unnest(results) %>%
  filter(term %in% c('Level1', 'Level3'))
```


## Fig 3: Clue 2

Here we restrict analyses to only those trials where at least one of the target words was guessed correctly.

```{r}

clue2_main = read_csv(here("../data/descriptive_precomputed.csv")) %>% 
  filter(Clue2 != "na" & Clue2 != "") %>%
  rowwise() %>% 
  mutate(w1_guessed = as.numeric(str_detect(Word1, P2W1) | str_detect(Word1, P2W2)),
         w2_guessed = as.numeric(str_detect(Word2, P2W1) | str_detect(Word2, P2W2)),
         A1PartCorrect = ifelse( (w1_guessed == 1 & w2_guessed == 0), "w2 incorrect",
                                 ifelse((w1_guessed == 0 & w2_guessed == 1),
                                     "w1 incorrect", "both incorrect"))) %>%
  filter(A1PartCorrect != "both incorrect")
  
## glove
clue2_main$glove_c2_sim_guessed = ifelse(clue2_main$w1_guessed == 1, 
                                      clue2_main$glove_c2w1_sim, clue2_main$glove_c2w2_sim)
clue2_main$glove_c2_sim_unguessed = ifelse(clue2_main$w1_guessed == 0, 
                                      clue2_main$glove_c2w1_sim, clue2_main$glove_c2w2_sim)
clue2_main$glove_c2_sim_composite = clue2_main$glove_c2_c1avg_sim

## swow

clue2_main$swow_c2_sim_guessed = ifelse(clue2_main$w1_guessed == 1, 
                                      clue2_main$swow_c2w1_sim, clue2_main$swow_c2w2_sim)
clue2_main$swow_c2_sim_unguessed = ifelse(clue2_main$w1_guessed == 0, 
                                      clue2_main$swow_c2w1_sim, clue2_main$swow_c2w2_sim)
clue2_main$swow_c2_sim_composite = clue2_main$swow_c2_c1avg_sim

## bert

clue2_main$bert_c2_sim_guessed = ifelse(clue2_main$w1_guessed == 1, 
                                      clue2_main$`bert-sum_c2w1_sim`,
                                      clue2_main$`bert-sum_c2w2_sim`)
clue2_main$bert_c2_sim_unguessed = ifelse(clue2_main$w1_guessed == 0, 
                                      clue2_main$`bert-sum_c2w1_sim`,
                                      clue2_main$`bert-sum_c2w2_sim`)
clue2_main$bert_c2_sim_composite = clue2_main$`bert-sum_c2_c1avg_sim`

## selecting only similarities
clue2_sims = clue2_main %>% select(Subject, Level, Word1, Word2, Clue1, Clue2,
                             glove_c2_sim_guessed:bert_c2_sim_composite) %>%
  pivot_longer(cols = -c(Subject, Word1, Word2, Level, Clue1, Clue2), names_to = 'comparison')%>%
   separate(comparison, into = c("Representation", "Clue", "Sim", "Type"))

## plot of clue 2 against guessed/unguessed/composite

Rmisc::summarySE(clue2_sims,
                             measurevar = "value",
                          groupvars = c("Representation","Type", "Clue"), na.rm = TRUE)%>%
mutate(Representation = fct_recode(Representation, 
                                     BERT = "bert", GloVe = "glove", SWOW = "swow"),
         Representation = fct_relevel(Representation, "BERT", "SWOW", "GloVe"),
       Type = fct_relevel(Type, "guessed", "unguessed", "composite"),
         Type = fct_recode(Type, centroid = "composite", `guessed\nword` = "guessed",
                           `unguessed\nword` = "unguessed")) %>%
ggplot(aes(x = Type, y = value,
           group = Representation, color = Representation)) + 
  geom_point(size = 2) + 
  geom_line(size = 1)+
 geom_errorbar(aes(ymin=value - ci, ymax=value + ci), 
             size = 0.5, width=0, 
                  color = "gray26", position = position_dodge(0))+
 theme_few()+
  scale_color_solarized()+
    labs(x = "", y = "cosine similarity") +
    ggtitle("Clue 2")+
    theme_few()+
  ylim(.2, .8)+
  ggtitle("second clue to target words\nand centroid")+
      theme(aspect.ratio = 1)+
   theme( axis.text.x = element_text( size = rel(2)),
         axis.text.y = element_text(size = rel(2)),
         strip.text.y = element_text(face = "bold", size = rel(1.3)),
     axis.title = element_text( size = rel(2)),
          legend.title = element_text( size = rel(2)),
     legend.text = element_text( size = rel(1.8)),
         plot.title = element_text(hjust = .5, size = rel(1.8)),
         strip.text.x = element_text(face = "bold", size = rel(1)))

## plot of clue2 against composite and easy/med/hard

Rmisc::summarySE(clue2_sims,
                             measurevar = "value",
                    groupvars = c("Representation", "Level", "Type", "Clue"), na.rm = TRUE) %>%
filter(Type == "composite") %>%
  mutate(Representation = fct_recode(Representation, 
                                     BERT = "bert", GloVe = "glove", SWOW = "swow"),
         Representation = fct_relevel(Representation, "BERT", "SWOW", "GloVe")) %>%
    mutate(Level = fct_relevel(Level, "Easy", "Medium", "Hard"),
           Level = fct_recode(Level, 
  `close\npair` = "Easy", `medium\npair` = "Medium", `distant\npair` =  "Hard")) %>%
ggplot(aes(x = Level, y = value,
           group = Representation, color = Representation)) + 
  geom_point(size = 2) + 
  geom_line(size = 1)+
 geom_errorbar(aes(ymin=value - ci, ymax=value + ci), 
             size = 0.5, width=0, 
                  color = "gray26", position = position_dodge(0))+
 theme_few()+
  scale_color_solarized()+
    labs(x = "", y = "cosine similarity") +
    ggtitle("Clue 2")+
    theme_few()+
    ylim(.2, .8)+
  ggtitle("second clue to centroid across\ntarget pair distance")+
      theme(aspect.ratio = 1)+
   theme( axis.text.x = element_text( size = rel(2)),
         axis.text.y = element_text(size = rel(2)),
         strip.text.y = element_text(face = "bold", size = rel(1.3)),
     axis.title = element_text( size = rel(2)),
          legend.title = element_text( size = rel(2)),
     legend.text = element_text( size = rel(1.8)),
         plot.title = element_text(hjust = .5, size = rel(1.8)),
         strip.text.x = element_text(face = "bold", size = rel(1)))

```

## Clue 2 Stats

```{r}
## ranef of whether second clue is closer to centroid/w1/w2

clue2_ranef_data = clue2_sims %>%
unite("wordpair", Word1, Word2, sep = '-') %>%
  mutate(Type = fct_relevel(Type, "guessed", "unguessed", "composite"))%>%
  nest(-Representation) %>% 
  mutate(results = map(data, ~ {
     lmer(value ~ Type + (1 | Subject) + (1|wordpair), 
          contrasts = list(Type = contr.treatment(3, base = 2)),
          data = .) %>% 
      broom.mixed::tidy()
    })) %>%
  unnest(results) %>%
  select(-data)%>%
  filter(term %in% c('Type1', 'Type3'))

## ranef of clue2 proximity to centroid as a function of difficulty

clue2_sims %>%
unite("wordpair", Word1, Word2, sep = '-') %>%
  filter(Type == "composite") %>%
    mutate(Level = fct_relevel(Level, "Easy", "Medium", "Hard"))%>%
  nest(-Representation) %>% 
  mutate(results = map(data, ~ {
     lmer(value ~ Level + (1|wordpair), 
          contrasts = list(Level = contr.treatment(3, base = 2)),
          data = .) %>% 
      broom.mixed::tidy()
    })) %>%
  unnest(results) %>%
  select(-data)%>%
  filter(term %in% c('Level1',  'Level3'))

```

# Model comparison

Import the combined speaker predictions for all models (generated by `model_results.ipynb` notebook).

```{r}
board_predictions <- read_csv("speaker_boardfunc_df_ranks_softmax.csv") %>%
  right_join(c_clues %>% select(wordpair, Clue1, clueCount)) %>%
  filter(!is.na(Model)) %>%
  mutate(Model = ifelse(Model == 'SWOW-RW', 'swow', tolower(Model))) %>%
  unite(Model, Model, alpha, sep = '-alpha') %>%
  select(-X1, -boardnames)

rsa_predictions <- read_csv("speaker_ranks.csv") %>%
  mutate(Model = paste0(representation, '-alphaRSA'),
         prag_speaker_rank = gsub('\\[', '', prag_speaker_rank),
         prag_speaker_rank = as.numeric(gsub('\\]', '', prag_speaker_rank))) %>%
  rename(clue_score = prag_speaker_probs,
         clue_rank = prag_speaker_rank) %>%
  select(Model, wordpair, Clue1, clue_score, clueCount, clue_rank)

speaker_compiled <- bind_rows(board_predictions, rsa_predictions) %>%
  mutate(wordpair = gsub(' - ', '-', wordpair)) %>%
  filter(!is.na(clue_score))
```

Import the combined listener predictions for all models (generated by `model_results.ipynb` notebook).

```{r}
guesser_compiled <- read_csv('guesser_scores.csv') %>%
  mutate(true_wordpair = gsub( " ", "", wordpair)) %>%
  group_by(representation, Clue1, wordpair) %>%
  mutate(prag_rank = dense_rank(-prag_likelihood),
         literal_rank = dense_rank(-literal_likelihood),
         literal_top_rank = literal_rank == 1, 
         prag_top_rank = prag_rank == 1) %>%
  right_join(c, by = c('Clue1', 'true_wordpair')) %>%
  filter(!is.na(literal_top_prediction)) %>%
  rowwise() %>%
  filter(possible_wordpair == chosen_wordpair1 || possible_wordpair == chosen_wordpair2) %>%
  distinct()
```

## Speaker Predictions

### Table 3: Log likelihoods 

we look at log likelihoods as the most fundamental metric of model comparison

```{r}
speaker_compiled %>% 
  mutate(ll = log(clue_score)) %>%
  group_by(Model) %>%
  summarize(ll = sum(ll * clueCount, na.rm = T)) %>%
  arrange(-ll) %>%
  separate(Model, into = c('model', 'alpha'), sep = '-alpha') %>%
  mutate(category = case_when(alpha == 'RSA' ~ 'RSA',
                              alpha == '1' ~ 'target',
                              TRUE ~ 'board')) %>%
  group_by(model, category) %>%
  filter(max(ll) == ll)

```

### Average ranks (Figure 5)

for a more interpretable measure, we look at the average position of the empirical scores in the model's rankings. this gives some 'absolute' sense of how good these models are doing. Note that we're using a weighted average here. 

```{r}
speaker_compiled %>%
  group_by(Model, wordpair) %>%
  summarize(avg_rank = sum(clue_rank * clueCount)/sum(clueCount)) %>%
  separate(Model, into = c('model', 'alpha'), sep = '-alpha') %>%
  mutate(model = ifelse(model == 'bert-sum', 'BERT', ifelse(model == 'swow', 'SWOW', model))) %>%
  filter(alpha %in% c(0, 0.2, 0.4, 0.6, 0.8, 1, 'RSA')) %>%
  ggplot(aes(x = alpha, y = log(avg_rank), color = alpha == 'RSA')) +
    geom_jitter(width = 0.1, height = 0) +
    geom_boxplot(alpha = .5) +
    geom_hline(yintercept = c(0, log(12100)), linetype = 'dotted') +
    labs(y = '(log) rank of clues') +
    facet_grid(~ model) +
    ylim(0, 9.5) +
    theme_few() +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),
         aspect.ratio = 1.5,
         legend.position = 'none') 

ggsave('avg_ranking.pdf', width = 4, height = 2.5, units = 'in')
```

### Average ranks (Table 3)

```{r}
speaker_compiled %>%
  group_by(Model) %>%
  tidyboot_mean(sum(clue_rank * clueCount)/sum(clueCount)) %>%
  separate(Model, into = c('model', 'alpha'), sep = '-alpha') %>%
  mutate(category = case_when(alpha == 'RSA' ~ 'RSA',
                              alpha == '1' ~ 'target',
                              TRUE ~ 'board')) %>%
  group_by(model, category) %>%
  filter(min(empirical_stat) == empirical_stat)
```

### Top-5 accuracy  (Table 3)

for the table, we look up the proportion of items where the single empirically most frequent word matches the model's single top prediction (well, ok, technically we're looking at the top 5 for simplicity because we didn't exclude the target words themselves from the ranking lists...)

```{r}
# tidyboot_mean is bootstrapping 95% CIs
speaker_compiled %>%
  mutate(top = clue_rank < 5) %>%
  group_by(Model) %>%
  tidyboot_mean(sum(top * clueCount)/sum(clueCount)) %>%
  separate(Model, into = c('type', 'alpha'), sep = '-alpha') %>%
  mutate(category = case_when(alpha == 'RSA' ~ 'RSA',
                              alpha == '1' ~ 'target',
                              TRUE ~ 'board')) %>%
  group_by(type, category) %>%
  filter(max(empirical_stat) == empirical_stat)
```

look at some examples of modal clues (e.g. *glove* models predict 'blow' and 'boat' better but *swow* predicts 'math' and 'test' better)

```{r}
# These are the ones where one of our models gave a top rank
speaker_compiled %>% 
  group_by(wordpair) %>%
  top_n(1, clueCount) %>%
  select(wordpair, clue_rank, Model, Clue1) %>%
  filter(Model %in% c('glove-alpha1', 'swow-alpha1', 'glove-alphaRSA', 'swow-alphaRSA', 'bert-sum-alpha1', 'bert-sum-alphaRSA')) %>%
  filter(!str_detect(wordpair, Clue1)) %>%
  filter(clue_rank == 1)

speaker_compiled %>% 
  filter(wordpair %in% c("exam-algebra")) %>%
  arrange(wordpair, clue_rank)

speaker_compiled %>% 
  filter(wordpair %in% c("feet-chapel")) %>%
  arrange(wordpair, clue_rank)

```

## Guesser predictions

### Log-likelihood (Table 4)

```{r}
guesser_compiled %>% 
  mutate(prag_ll = log(prag_likelihood),
         literal_ll = log(literal_likelihood)) %>%
  gather(process_type, ll, prag_ll, literal_ll) %>%
  group_by(representation, process_type) %>%
  summarize(ll = sum(ll, na.rm = T)) %>%
  arrange(-ll)
```

### Ranks (Table 4)

```{r}
guesser_compiled %>%
  group_by(representation) %>%
  select(possible_wordpair, chosen_wordpair1, Clue1, ends_with('rank')) %>%
  gather(model, rank, literal_rank, prag_rank) %>%
  group_by(representation, model) %>%
  tidyboot_mean(rank)
```

### Accuracy (Table 4)

```{r}
guesser_compiled %>%
  # group_by(representation, Board, Clue1, chosen_wordpair1, literal_top_rank, prag_top_rank) %>%
  # tally() %>%
  # filter(n == max(n)) %>%
  group_by(representation) %>%
  select(chosen_wordpair1, Clue1, literal_top_rank, prag_top_rank) %>%
  gather(model, top, literal_top_rank, prag_top_rank) %>%
  group_by(representation, model) %>%
  tidyboot_mean(top)
```

# Supplemental

## Performance across time

are people getting better at this?

```{r}
## each dyad has 30 rows corresponding to the 30 rounds they played
## is performance getting better over time within a dyad?
c = read.csv("cogsci_descriptive.csv", header = TRUE, sep = ",")
c$wordpair = paste(c$Word1, "-", c$Word2)

# Attempts to guess the targets
library(tidyverse)
library(dplyr)
c$Attempts = ifelse(c$Player2.ACC == 1, 1, ifelse(c$Player2SecondAnswer.ACC == 1, 2, 3))

acc = c %>% select(Subject, Trial, wordpair,  Attempts) %>% arrange(Subject, Trial) %>% 
  group_by(Trial) %>%
  summarise_at(vars(Attempts), mean) 
acc = acc %>%
  mutate(Trials = as.numeric(rownames(acc)))

Hmisc::rcorr(acc$Trials, acc$Attempts) ## n.s. correlation

# Time taken by Speaker to generate clues
speaker_rt = c %>% select(Subject, Trial, wordpair, Player1.RT) %>% arrange(Subject, Trial) %>%
  group_by(Trial) %>%
  summarise_at(vars(Player1.RT), mean) 
speaker_rt = speaker_rt%>%
  mutate(Trials = as.numeric(rownames(speaker_rt)))
Hmisc::rcorr(speaker_rt$Trials, speaker_rt$Player1.RT) ## n.s. correlation

# Time taken by Guesser to make first guesses

guesser_rt = c %>% select(Subject, Trial, wordpair, Player2.RT) %>% arrange(Subject, Trial) %>%
  group_by(Trial) %>%
  summarise_at(vars(Player2.RT), mean)
guesser_rt = guesser_rt %>%
  mutate(Trials = as.numeric(rownames(guesser_rt)))
Hmisc::rcorr(guesser_rt$Trials, guesser_rt$Player2.RT) ## n.s. correlation

```

## Error analysis

it's interesting that RSA model seems to do slightly worse than the 'pure' alpha = 1 model that completely ignores distractors. let's look at some specific cases to try to understand the errors the RSA model might be making. 

first of all, we see that predictions are pretty highly correlated (r = 0.71) at the item-level.

```{r}
speaker_compiled %>%
  filter(Model %in% c('swow-alpha1', 'swow-alphaRSA')) %>%
  group_by(Model, wordpair) %>%
  summarize(avg_rank = sum(clue_rank * clueCount)/sum(clueCount)) %>%
  select(Model, avg_rank, wordpair) %>%
  spread(Model, avg_rank) %>%
  select(-wordpair) %>%
  ggpairs(mapping = aes(alpha = 0.01), lower = list(continuous = "smooth"), progress = F) +
    ggthemes::theme_few()
ggsave('correlations.pdf', width = 10, height = 10, units = 'in')
```

but RSA is *terrible* for some clues. for pairs like 'communicate - cooking', it seems like RSA doesn't want to produce clues like 'food' or 'kitchen' that are good for 'cooking' but not 'communicate'. it ranks these almost at the very bottom. this is more similar to what the alpha = 0.8 model is doing. 

```{r}
speaker_compiled %>%
  filter(Model %in% c('swow-alpha1', 'swow-alpha0.8', 'swow-alphaRSA')) %>%
  filter(wordpair == 'communicate - cooking') %>%
  select(Model, Clue1, clue_rank, wordpair) %>%
  spread(Model, clue_rank)
```

## Clue 2 (descriptive)

```{r}
## plot of clue 1 to midpoint and w1w2

c.sims %>%
  group_by(Subject, wordpair, representation, comparison) %>%
  pivot_wider(names_from = comparison, values_from = sim) %>%
  group_by(representation, wordpair, Subject) %>%
  transmute(c2w1 = min(c2w1_sim, c2w2_sim),
            c2w2 = max(c2w1_sim, c2w2_sim),
            midpoint = c2_w1w2avg_sim,
            centroid = c2_c1avg_sim) %>%
  pivot_longer(cols = -c(Subject, wordpair, representation), names_to = 'comparison') %>%
  group_by(representation, comparison) %>%
  summarise_at(vars(value), mean, na.rm = TRUE)  %>%
  ungroup()  %>%
  mutate(comparison = fct_relevel(comparison, "c2w1", "c2w2", "midpoint", "centroid")) %>%
  ggplot(aes(x = comparison, y = value, color = representation, group = representation)) + 
    geom_point(size = 2) + 
    geom_line(size = 1)+
    scale_color_solarized() +
    labs(x = "", y = "cosine similarity") +
  ggtitle("second clue to w1, w2, and midpoint")+
    theme_few()+
    theme(aspect.ratio = 1)

## plot of clue2 to midpoint across easy/med/hard

c.sims %>%
  group_by(Subject, Level, wordpair, representation, comparison) %>%
  pivot_wider(names_from = comparison, values_from = sim) %>%
  group_by(representation, Level, wordpair, Subject) %>%
  transmute(c2w1 = min(c2w1_sim, c2w2_sim),
            c2w2 = max(c2w1_sim, c2w2_sim),
            midpoint = c2_w1w2avg_sim,
            centroid = c2_c1avg_sim) %>%
  pivot_longer(cols = -c(Subject, Level, wordpair, representation), names_to = 'comparison') %>%
  group_by(representation, Level, comparison) %>%
  summarise_at(vars(value), mean, na.rm = TRUE)  %>%
  ungroup()  %>%
  mutate(comparison = fct_relevel(comparison, "c2w1", "c2w2", "midpoint"),
         Level = fct_relevel(Level, "Easy", "Medium", "Hard")) %>%
  filter(comparison %in% c("midpoint", "centroid"))  %>%
  ggplot(aes(x = Level, y = value, color = representation, group = representation)) + 
    geom_point(size = 2) + 
    geom_line(size = 1)+
    scale_color_solarized() +
    labs(x = "word-pair difficulty", y = "cosine similarity") +
  ggtitle("second clue to midpoint/centroid")+
    theme_few()+
  facet_wrap(~comparison)+
    theme(aspect.ratio = 1)
```