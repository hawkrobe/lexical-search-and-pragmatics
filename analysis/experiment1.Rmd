---
title: "Experiment 1 (Full Two-player Connector)"
output:
  pdf_document: default
  html_document:
    df_print: paged
editor_options: 
  chunk_output_type: console
---

# imports

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(tidyverse)
library(ggthemes)
library(tidyboot)
library(here)
library(jsonlite)
```

# speaker selections

```{r}

exp1_speaker_df <- read_csv(here("../data/exp1/model_output/speaker_df.csv"))

# recompute levels based on listener accuracy

listener = read_csv(here("../data/exp1/raw_listener.csv")) %>%
  unite("wordpair", Word1, Word2, sep = "-") %>% 
  group_by(wordpair) %>%
  summarise(accuracy = mean(Player2.ACC),
            speakerRT = mean(Player1.RT))%>%
  mutate(listener_tertile = ntile(accuracy, 3),
         speaker_tertile = ntile(speakerRT,3)) %>%
  mutate(Level = ifelse(speaker_tertile == 1, "Easy", ifelse(speaker_tertile == 2, "Medium", "Hard")))

car::Anova(lm(data=listener, accuracy ~ Level))
listener %>% group_by(Level) %>% summarise(mean(accuracy))

speaker = exp1_speaker_df %>% select(-Level) %>%
  left_join(listener %>% select(wordpair, Level)) %>%
  mutate(Level = as.factor(Level)) %>%
  mutate(Level = fct_relevel(Level, "Easy", "Medium", "Hard"))
```

# specific wordpair-level parameters
```{r}
wordpair_params = speaker %>% group_by(wordpair) %>% 
  mutate(LL = log(prob)) %>%
  group_by(wordpair, Level, model, alpha, cost_fn, costweight) %>%
  summarise(sumLL = sum(LL, na.rm = TRUE)) %>%
  ungroup() %>%
  group_by(wordpair) %>%
  filter(sumLL == max(sumLL))
```

# model comparison plot
```{r}
beste1_models = speaker %>%
  filter(model %in% c("RSAcdf", "RSAfreq"))%>%
  mutate(type = ifelse(costweight == 0, "no search",
          ifelse(costweight == 1, "no pragmatics","search + pragmatics"))) %>%
  mutate(LL = log(prob)) %>%
  group_by(type, Level, model, alpha, cost_fn, costweight) %>%
  summarise(sumLL = sum(LL, na.rm = TRUE)) %>%
  ungroup() %>%
  group_by(type,Level, cost_fn ) %>%
  filter(sumLL == max(sumLL))

beste1_models %>%
  ggplot(aes(x = as.numeric(cost_fn), y = sumLL)) +
  geom_line(stat = 'identity', data = . %>% filter(model == 'RSAcdf'),
            aes(group = type, color = type)) +
  geom_hline(aes(yintercept = sumLL, color = type), 
             linetype = 'dashed', 
             data = . %>% filter(model == 'RSAfreq' & type != "no search")) +
  geom_point(data = . %>% group_by(Level) %>% filter(sumLL == max(sumLL)))+
  facet_wrap(~ Level, scales = 'free')+
  scale_x_log10(limits = c(1, 10000)) +
  scale_color_manual(values = c('#60BD68', '#D8CF23', '#CC253D')) +
    theme_few() +
    labs(x = "search budget") +
    theme(aspect.ratio = 1)

ggsave(here('plots/exp1_modelcomparison.pdf'), units = 'in', height = 3, width = 10)
```

# table 2 results
```{r}
View(beste1_models %>%
       mutate(type = as.factor(type)) %>%
       mutate(type = fct_relevel(type, "search + pragmatics", "no pragmatics", "no search")) %>%
       filter(model == "RSAcdf") %>%
  group_by(Level,type, model) %>%
  filter(sumLL == max(sumLL)) %>%
  arrange(Level, model) %>%
    slice_head()) 
```
# cost heatmap
```{r}
speaker %>%
  filter(model == "RSAcdf") %>%
  mutate(modeltype = ifelse(costweight == 1, "no pragmatics",
          ifelse(costweight == 0, "no search", "search + pragmatics"))) %>%
  filter(modeltype != "no search") %>%
  group_by(modeltype, Level, model, alpha, cost_fn, costweight) %>%
  summarize(loglik = sum(log(prob), na.rm = T)) %>%
  rename(budget = cost_fn) %>% 
  group_by(Level, modeltype, budget, costweight) %>%
  #mutate(loglik = ifelse(loglik < -5000, -5000, loglik)) %>%
  filter(loglik == max(loglik)) %>%
  ungroup() %>%
  # complete(Level, modeltype, budget, costweight, fill = list(loglik = NA)) %>%
  # group_by(Level, modeltype, costweight) %>%
  # mutate(loglik = ifelse(is.na(loglik), mean(loglik, na.rm = T), loglik)) %>%
  ggplot(aes(x = costweight, y = as.numeric(budget), fill = loglik)) +
    geom_tile(stat = 'identity', height=.34, width=.33) +
    geom_tile(color = 'black', stat = 'identity', height=.34, width=.33, size = 1.1,
              data = . %>% group_by(Level) %>% filter(loglik == max(loglik))) +
    facet_grid(Level ~ modeltype) +
    scale_x_log10() +
    scale_y_log10() +
    scale_fill_gradient2(midpoint = -4250) +
    theme_few() +
    labs(y = "search budget", "cost weight") +
    theme(aspect.ratio = 1)

ggsave(here("plots/exp1_heatmap.pdf"), units = 'in', width = 6, height = 5)
```

# blended model

```{r}
logsumexp <- function(x) {
  y <- max(x)
  y + log(sum(exp(x - y)))
}

softmax <- function(x) {
  exp(x - logsumexp(x))
}

blended_raw <- read_csv(here("data/exp1/model_output/blended.csv")) %>%
  select(-n) %>%
  filter(!is.na(bias_weight)) %>%
  rename(correctedClue = Word) %>%
  group_by(wordpair) %>%
  complete(correctedClue, bias_weight, step, fill = list(cdf = 0))
```

```{r}
blended <- blended_raw %>%
  group_by(wordpair, step,  bias_weight) %>%
  mutate(prob = softmax(6 * cdf)) %>%
  ungroup() %>%
  right_join(d_exp1, relationship = "many-to-many") %>%
  filter(!is.na(bias_weight)) %>%
  group_by(Level, bias_weight, step) %>%
  summarize(loglik = sum(log(prob), na.rm = TRUE))

blended %>%
  ungroup() %>%
  filter(loglik == max(loglik))
```

```{r}
blended %>%
  ggplot(aes(x = as.numeric(step), y = loglik,
             color = bias_weight, group = bias_weight)) +
    geom_line(aes(linetype = bias_weight %in% c(0,1), 
                  alpha = bias_weight %in% c(0, 0.01, 1)), 
              show.legend = c(colour = TRUE, linetype = FALSE, alpha=FALSE)) +
    facet_wrap(Level ~ ., scales = 'free') +
    scale_x_log10() +
    theme_few() +
    labs(x = "search budget", y = "log likelihood", color = "bias") +
    theme(aspect.ratio = 1, legend.position = 'right')

ggsave('blended.pdf', height = 15, width = 15)
```

## examine CDFs

```{r}
blended_raw %>%
  right_join(d_exp1, relationship = "many-to-many") %>%
  distinct() %>%
  filter(wordpair == 'lion-tiger') %>%
  ggplot(aes(x = as.numeric(step), y = cdf, group = correctedClue,
             linetype = correctedClue %in% c('cat', 'animal'))) +
    geom_line() +
    facet_wrap(bias_weight ~ ., scales = 'free') +
    scale_x_log10() +
#    ylim(0, 1) +
    theme_few() +
    labs(x = "search budget") +
    theme(aspect.ratio = 1, legend.position = 'none')

ggsave('blended.pdf', height = 15, width = 15)
```


<!-- ## OTHER CODE -->

<!-- ## Compute costs for the whole vocab -->

<!-- ```{r} -->
<!-- pairs <- d.exp1 %>% pull(wordpair) %>% unique() -->

<!-- getWalks <- function(pair) { -->
<!--   paste0('data/exp1/model_output/', pair, '-walks.json', collapse = '') %>% -->
<!--     here() %>% -->
<!--     fromJSON() %>%  -->
<!--     as_tibble() %>% -->
<!--     mutate(step = row_number()) %>% -->
<!--     gather(walk, Word, -step) %>% -->
<!--     mutate(walk = gsub('walk-', '', walk), -->
<!--            wordpair = pair) %>%  -->
<!--     group_by(walk, Word, wordpair) %>% -->
<!--     filter(step == first(step)) %>% -->
<!--     group_by(step, Word, wordpair) %>% -->
<!--     tally() %>% -->
<!--     ungroup() %>% -->
<!--     group_by(wordpair) %>% -->
<!--     complete(Word, step = 2**seq(13), fill = list(n=0)) %>% -->
<!--     group_by(Word, wordpair) %>% -->
<!--     arrange(step) %>% -->
<!--     mutate(cdf = cumsum(n)/2000) %>% -->
<!--     filter(step %in% 2**seq(13)) %>% -->
<!--     write_csv(here(paste0('data/exp1/model_output/', pair, '-cdf.csv',collapse = ''))) -->
<!-- } -->

<!-- library(furrr) -->
<!-- library(progressr) -->

<!-- plan(multisession, workers = 8) -->
<!-- cdfs <- with_progress({ -->
<!--     p <- progressor(steps = length(pairs)) -->
<!--     future_walk(pairs, ~{p(); getWalks(.x)})  -->
<!-- }) -->
<!-- ``` -->

<!-- # Reformat -->

<!-- ```{r} -->
<!-- complete_df <- read_csv(here('data/exp1/model_input/vocab.csv')) %>% -->
<!--   left_join(read_csv(here('data/exp1/model_output/ranks.csv'))) %>% -->
<!--   mutate(single = (w1_index_walk + w2_index_walk)/2) %>% -->
<!--   select(-`...1`, -index, -Occurrences,-Length) %>% -->
<!--   gather(measure, value, -Word, -wordpair) %>% -->
<!--   complete(Word, wordpair, measure, fill = list(value = 0)) %>% -->
<!--   filter(!is.na(step), !is.na(wordpair)) -->

<!-- read_csv(here('data/exp1/model_input/vocab.csv')) %>% -->
<!--   left_join(complete_df) %>% -->
<!--   select(Word,wordpair,measure,value) %>% -->
<!--   write_csv(here('data/exp1/model_output/freqs_long.csv')) -->
<!-- ``` -->