{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "54ZMyOWfsEGS"
   },
   "source": [
    "# importing embeddings, vocabulary, & functions file\n",
    "We load the embeddings, vocabulary, and all the search functions. The functions have been predefined and are stored in the search-models subdirectory in the github repository, so we directly load them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "BxoEmRAHPuDn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "from search import SWOW\n",
    "from pragmatics import RSA, nonRSA, utils\n",
    "import walker \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import walk data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab is 12218 words\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# with open('../data/walk_data/intersection_candidates.json') as json_file:\n",
    "#     intersection_dict = json.load(json_file)\n",
    "# with open('../data/walk_data/union_candidates.json') as json_file:\n",
    "#     union_dict = json.load(json_file)\n",
    "# corrections = pd.read_csv('../data/corrections.csv')\n",
    "vocab = pd.read_csv(\"../data/vocab.csv\").rename(columns={\"Word\": \"vocab_word\"})\n",
    "print(f\"vocab is {len(vocab)} words\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1: candidate generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_study_data = (\n",
    "    pd.read_csv('../data/exp1/e1_data.csv')\n",
    "      .melt(id_vars=['clueGiverID' , 'wordpair_id', 'Level', 'clueFinal'], \n",
    "            value_vars=[\"clueOption1\", \"clueOption2\", \"clueOption3\", \"clueOption4\", \n",
    "                        \"clueOption5\", \"clueOption6\", \"clueOption7\", \"clueOption8\"])\n",
    "      .rename(columns={\"value\": \"Clue1\"})\n",
    ")\n",
    "generation_study_data = utils.apply_corrections(generation_study_data, corrections, vocab)\n",
    "generation_study_data = (\n",
    "    generation_study_data.dropna()\n",
    "      .drop_duplicates()\n",
    "      .groupby(['clueGiverID','wordpair_id',  'Level', 'clueFinal'], \n",
    "               as_index=False)['correctedClue']\n",
    "      .agg(','.join)\n",
    ")\n",
    "generation_study_data['clue_list'] = generation_study_data['value'].str.split(',')\n",
    "common_candidates = utils.get_common_candidates(union_dict, intersection_dict, generation_study_data, '../data/exp1/e1_common_candidates.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2: original connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e2_data = pd.read_csv(\"../data/exp2/e2_empirical_clues.csv\", encoding= 'unicode_escape')\n",
    "target_df = pd.read_csv(\"../data/targets.csv\")\n",
    "print(target_df.head())\n",
    "representations = {}\n",
    "representations['swow'] = pd.read_csv(\"../data/swow_associative_embeddings.csv\").transpose().values\n",
    "print(f\"embeddings are shaped:\", representations['swow'].shape)\n",
    "with open('../data/exp2/e2_boards.json', 'r') as json_file:\n",
    "    e2_boards = json.load(json_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## non-RSA\n",
    "\n",
    "Here we import the candidates from the json files and first compute how the non-RSA model would rank these candidates for each budget level. Next, we merge these obtained probabilities with the actual behavioral data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "board_optimal_params = {\n",
    "    'swow' : (23.488850322875496, 1), # -13204\n",
    "    'glove' : (20.952928531665275, 1), # -15774.814774380024)\n",
    "    'bert-sum' : (19.983835225540847, 0.787924454045298),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e2_data = nonRSA.apply_corrections(e2_data, corrections, vocab)\n",
    "e2_data['wordpair'] = e2_data['wordpair'].str.replace(' ', '')\n",
    "e2_data = e2_data.merge(target_df, on='wordpair', how='left')\n",
    "e2_data.to_csv(\"../data/exp2/e2_corrected.csv\", index=False)\n",
    "e2_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidateprobs_nonRSA  = nonRSA.get_nonRSA_union_int(union_dict, intersection_dict, target_df, e2_boards, board_optimal_params, vocab, representations, e2_data, '../data/exp2/nonRSAprobs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsa_optimal_params = {\n",
    "    'swow' : (25.1522030761838, 0.03863169001849234),\n",
    "    'glove' : (82.83019661384789, 0.9997249702731884),\n",
    "    'bert-sum' : (29.709602301411962, 0.031659060110267576), #-17533\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidateprobs_RSA  = RSA.get_RSA_union_int(union_dict, intersection_dict, target_df, e2_boards, rsa_optimal_params, vocab, representations, e2_data, '../data/exp2/RSAprobs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 3: targeted endorsements\n",
    "Here, we look at how the models predict specific target endorsements from the candidate set provided to the speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/exp3/e3_boards.json', 'r') as json_file:\n",
    "    e3_boards = json.load(json_file)\n",
    "e3_stimuli = pd.read_csv('../data/exp3/e3_stimuli.csv')\n",
    "## pass through corrections file\n",
    "e3_stimuli = nonRSA.apply_corrections(e3_stimuli, corrections, vocab)\n",
    "# create column that records whether correctedClue in vocab or not\n",
    "e3_stimuli['correctedClue_in_vocab'] = e3_stimuli['correctedClue'].isin(vocab['vocab_word'])\n",
    "# merge with target_df\n",
    "e3_stimuli = e3_stimuli.merge(target_df, on='wordpair', how='left')\n",
    "e3_stimuli.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## non-RSA probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e3_nonRSA = nonRSA.get_nonRSA_union_int(union_dict, intersection_dict, target_df, e3_boards, board_optimal_params, vocab, representations, e3_stimuli, '../data/exp3/nonRSAprobs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RSA probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e3_nonRSA  = RSA.get_RSA_union_int(union_dict, intersection_dict, target_df, e3_boards, rsa_optimal_params, vocab, representations, e3_stimuli, '../data/exp3/RSAprobs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# other code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for finding closest matches to a given word (spell-check)\n",
    "from english_words import english_words_set\n",
    "'imitate' in english_words_set\n",
    "import difflib\n",
    "difflib.get_close_matches('turbine', list(english_words_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expdata_long = e3_stimuli.groupby(['wordpair'], as_index=False)['value'].agg(','.join)\n",
    "expdata_long['clue_list'] = expdata_long['value'].str.split(',')\n",
    "expdata_long = expdata_long.merge(target_df, on = \"wordpair\")\n",
    "# count how many clues for each wordpair are not in vocab\n",
    "expdata_long['len_cluelist_not_in_vocab'] = expdata_long['clue_list'].apply(lambda x: len([e for e in x if e not in list(vocab.vocab_word)]))\n",
    "expdata_long_subset = expdata_long[expdata_long['len_cluelist_not_in_vocab'] == 0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qt6jgRIeM7YO"
   },
   "source": [
    "# Old: Dedicated functions for full vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5gKTDYETeKlH"
   },
   "outputs": [],
   "source": [
    "## create boards and merge with expdata\n",
    "combined_boards_df = pd.DataFrame(columns=['Experiment', 'Board','boardwords'])\n",
    "combined_boards_df[\"Experiment\"]  = [\"E1\"] * 10 + [\"E2\"] * 10\n",
    "combined_boards_df[\"Board\"] = [\"TrialList\" + str(i) for i in range(1,11)] * 2\n",
    "combined_boards_df[\"boardnames\"] = (['e1_board' + str(i) + '_words' for i in range(1,11)] \n",
    "                                  + ['e2_board' + str(i) + '_words' for i in range(1,11)])\n",
    "combined_boards_df[\"boardwords\"] = [boards[n] for n in combined_boards_df[\"boardnames\"]]\n",
    "\n",
    "expdata_new = pd.merge(expdata,combined_boards_df,on=['Board', 'Experiment'],how='left')\n",
    "expdata_new[\"wordpair\"] = expdata_new[\"Word1\"] + \"-\" + expdata_new[\"Word2\"]\n",
    "board_combos = {board_name : search_funcs.RSA.compute_board_combos(board_name, boards) for board_name in boards.keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FZOYctgjM5NM"
   },
   "source": [
    "## Non - RSA method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v8QEomkF4hmB"
   },
   "outputs": [],
   "source": [
    "board_optimal_params = {\n",
    "    'swow' : (23.488850322875496, 1), # -13204\n",
    "    'glove' : (20.952928531665275, 1), # -15774.814774380024)\n",
    "    'bert-sum' : (19.983835225540847, 0.787924454045298),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9r7kmGrcNCvB"
   },
   "outputs": [],
   "source": [
    "cluescoredf = nonRSA.speaker_targetboard_cluescores(['swow', 'glove'], board_optimal_params, board_combos, boards, list(vocab.vocab_word), vocab, representations, target_df, expdata_new)\n",
    "cluescoredf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67827P5C897A"
   },
   "source": [
    "## RSA Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IVrhPE-xN6I4"
   },
   "outputs": [],
   "source": [
    "rsa_optimal_params = {\n",
    "    'swow' : (25.1522030761838, 0.03863169001849234),\n",
    "    'glove' : (22.336514544537227, 0.039),\n",
    "    'bert-sum' : (29.709602301411962, 0.031659060110267576), #-17533\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ewq3Y68zSd_H"
   },
   "outputs": [],
   "source": [
    "pragmaticspeakerdf = search_funcs.RSA.get_speaker_df(representations, combined_boards_df,rsa_optimal_params, list(vocab.vocab_word), vocab, expdata_new, board_combos, target_df, boards)\n",
    "pragmaticspeakerdf.head()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPN3gA5HcesL1IibYO9jUBA",
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "search-models.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
