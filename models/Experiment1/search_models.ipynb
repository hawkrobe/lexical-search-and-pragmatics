{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/abhilasha-kumar/Connector/blob/master/search-models/search_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "54ZMyOWfsEGS"
   },
   "source": [
    "# Importing embeddings, vocabulary, & functions file\n",
    "We load the embeddings, vocabulary, and all the search functions. The functions have been predefined and are stored in the search-models subdirectory in the github repository, so we directly load them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "BxoEmRAHPuDn"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "import search_funcs\n",
    "import walker \n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u0svZpSd88Jz"
   },
   "source": [
    "\n",
    "We have the following models that can be run on full vocab/candidates and with/without pragmatics:\n",
    "1.   Target+Board (non-RSA)\n",
    "2.   Pragmatic speaker (RSA)\n",
    "\n",
    "We have the following search models that generate candidates:\n",
    "1.   Union (RW)\n",
    "2.   Intersection (RW)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "swow = search_funcs.SWOW('../../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random walks - T=2.25s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[   0,    1,    5, 2915, 3125,  218, 1941, 3139, 5456, 4832]],\n",
       "      dtype=uint32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "walker.random_walks(swow.graph, n_walks=1, walk_len=10, start_nodes=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random walks - T=2.62s\n",
      "Random walks - T=2.62s\n",
      "{'w1_visited_count': array([0., 0., 0., ..., 0., 0., 0.])}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'loc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [53]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m44\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mswow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munion_intersection_nwalks\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mman\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwoman\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Repos/connector-pragmatics-search/models/Experiment1/search_funcs.py:182\u001b[0m, in \u001b[0;36mSWOW.union_intersection_nwalks\u001b[0;34m(self, w1, w2, walk_len, n_walks)\u001b[0m\n\u001b[1;32m    179\u001b[0m v[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw1*w2\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m v[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw1_visited_count\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m*\u001b[39mv[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw2_visited_count\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    181\u001b[0m \u001b[38;5;66;03m## compute non-zero, i.e., all visited nodes \u001b[39;00m\n\u001b[0;32m--> 182\u001b[0m nonzero_w1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m[v[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw1_visited_count\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mvocab_word)\n\u001b[1;32m    183\u001b[0m nonzero_w2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(v\u001b[38;5;241m.\u001b[39mloc[v[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw2_visited_count\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mvocab_word)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m## compute union and intersection\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'loc'"
     ]
    }
   ],
   "source": [
    "np.random.seed(44)\n",
    "swow.union_intersection_nwalks('man', 'woman', 100, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ht74xlvhz7xv"
   },
   "source": [
    "# Candidate generation (Union & Intersection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ONVSCWUi166H"
   },
   "source": [
    "## keeping n_walks constant\n",
    "\n",
    "Here we keep the number of random walks constant = 1000 and compute the union and intersection between the two words' walks for different number of steps (determined via powers of 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hlh_haE7tazm"
   },
   "outputs": [],
   "source": [
    "## here we generate candidates for each of our wordpairs: stored in target_df\n",
    "candidates_df = pd.DataFrame()\n",
    "\n",
    "# keep n_walks fixed to a large number\n",
    "n_walks = 1000\n",
    "\n",
    "for index, row in target_df.iterrows():\n",
    "  w1 = row[\"Word1\"]\n",
    "  w2 = row[\"Word2\"]\n",
    "  print(f\"for {w1} and {w2}\")\n",
    "  union_df, int_df = search_funcs.search.union_intersection(w1,w2, n_walks, swow_vocab, swowGraph)\n",
    "  print(f\"union/int calculation complete!\")\n",
    "  \n",
    "  union_df[\"Word1\"] = w1\n",
    "  union_df[\"Word2\"] = w2\n",
    "  union_df[\"n_walks\"] = n_walks\n",
    "  union_df[\"type\"] = \"union\"\n",
    "  union_df[\"wordpair\"] = row[\"wordpair\"]\n",
    "  \n",
    "\n",
    "  int_df[\"Word1\"] = w1\n",
    "  int_df[\"Word2\"] = w2\n",
    "  int_df[\"n_walks\"] = n_walks\n",
    "  int_df[\"type\"] = \"intersection\"\n",
    "  int_df[\"wordpair\"] = row[\"wordpair\"]\n",
    "\n",
    "  overall_df = pd.concat([union_df, int_df])\n",
    "\n",
    "  candidates_df = pd.concat([candidates_df, overall_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qupKzxhA2Jah",
    "tags": []
   },
   "source": [
    "## varying n_walks with powers of 2\n",
    "\n",
    "Here, we vary the number of walks in powers of 2 and then compute the union & intersection for different number of steps within those walks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PD_qKN4K2XOo"
   },
   "outputs": [],
   "source": [
    "## here we generate candidates for each of our wordpairs: stored in target_df\n",
    "\n",
    "#ncandidates_df = pd.DataFrame()\n",
    "\n",
    "# keep n_walks fixed to a large number\n",
    "walkmax = 4096 \n",
    "\n",
    "for index, row in target_df.iterrows():\n",
    "  w1 = row[\"Word1\"]\n",
    "  w2 = row[\"Word2\"]\n",
    "  print(f\"for {w1} and {w2}\")\n",
    "  wordpair = row[\"wordpair\"]\n",
    "  if(wordpair != \"exam-algebra\"):\n",
    "    union_df, int_df = search_funcs.search.union_intersection_nwalks(w1, w2, walkmax, swow_vocab, swowGraph)\n",
    "    print(f\"union/int calculation complete!\")\n",
    "    \n",
    "    union_df[\"Word1\"] = w1\n",
    "    union_df[\"Word2\"] = w2\n",
    "    union_df[\"type\"] = \"union\"\n",
    "    union_df[\"wordpair\"] = wordpair\n",
    "    \n",
    "    int_df[\"Word1\"] = w1\n",
    "    int_df[\"Word2\"] = w2\n",
    "    int_df[\"type\"] = \"intersection\"\n",
    "    int_df[\"wordpair\"] = wordpair\n",
    "\n",
    "    overall_df = pd.concat([union_df, int_df])\n",
    "\n",
    "    ncandidates_df = pd.concat([ncandidates_df, overall_df])\n",
    "    ncandidates_df.to_csv(parentfolder+'candidates_nwalks.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PLdI6v-6mrz2"
   },
   "source": [
    "## non-RSA method (varied walks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MeaK5YtQmuix"
   },
   "outputs": [],
   "source": [
    "ndf_filtered = ncandidates_df[(ncandidates_df['vocab_word'] != ncandidates_df[\"Word1\"]) & (ncandidates_df['vocab_word'] != ncandidates_df[\"Word2\"])]\n",
    "ndf_filtered[\"wordpair\"] = ndf_filtered[\"Word1\"] + \"-\"+ndf_filtered[\"Word2\"]\n",
    "## group by number of steps in the RW and union/intersection\n",
    "cwide = ndf_filtered.groupby(['wordpair', 'type', 'n_steps'], as_index=False)['vocab_word'].agg(','.join)\n",
    "cwide['clue_list'] = cwide['vocab_word'].str.split(',')\n",
    "cwide = cwide.merge(target_df, on = \"wordpair\")\n",
    "cwide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IcK3c37Y77Hl"
   },
   "source": [
    "## non-RSA method (fixed walks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tqvJhhc5Gw5p"
   },
   "outputs": [],
   "source": [
    "candidates_df = pd.read_csv(parentfolder+'swow_candidates.csv')\n",
    "candidates_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JUEhSzKwGbBk"
   },
   "outputs": [],
   "source": [
    "df_filtered = candidates_df[(candidates_df['vocab_word'] != candidates_df[\"Word1\"]) & (candidates_df['vocab_word'] != candidates_df[\"Word2\"])]\n",
    "\n",
    "df_filtered[\"wordpair\"] = df_filtered[\"Word1\"] + \"-\"+df_filtered[\"Word2\"]\n",
    "## group by number of steps in the RW and union/intersection\n",
    "cwide = df_filtered.groupby(['wordpair', 'type', 'n_steps'], as_index=False)['vocab_word'].agg(','.join)\n",
    "cwide['clue_list'] = cwide['vocab_word'].str.split(',')\n",
    "cwide = cwide.merge(target_df, on = \"wordpair\")\n",
    "cwide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M-p392Td76Zs"
   },
   "outputs": [],
   "source": [
    "board_optimal_params = {\n",
    "    'swow' : (23.488850322875496, 1), # -13204\n",
    "    'glove' : (20.952928531665275, 1), # -15774.814774380024)\n",
    "    'bert-sum' : (19.983835225540847, 0.787924454045298),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-PDXsZGA7_sl"
   },
   "outputs": [],
   "source": [
    "modelname = \"swow\"\n",
    "candidateprobs_nonRSA = pd.DataFrame()\n",
    "for alpha in np.arange(0,1.1, 0.1):\n",
    "  ## for a given alpha, compute the clue similarities at the board level \n",
    "  beta = board_optimal_params[modelname][0]\n",
    "  for index, row in cwide.iterrows():\n",
    "    boardname = row[\"boardnames\"]\n",
    "    cluelist = row[\"clue_list\"]\n",
    "    wordpair = row[\"wordpair\"]\n",
    "    board_probs = search_funcs.nonRSA.speaker_targetboard(boards[boardname], alpha, board_optimal_params[modelname][0], cluelist, representations, modelname, vocab, target_df)\n",
    "    ## obtain the probs for the specific wordpair\n",
    "    target_main = target_df.loc[target_df['boardnames'] == boardname]\n",
    "    target_main.reset_index(inplace = True)\n",
    "    wordpair_index = target_main.index[(target_main['wordpair'] == wordpair)].tolist()[0]\n",
    "    mainscores = board_probs[wordpair_index]\n",
    "\n",
    "    clue_board_df = pd.DataFrame({'alpha': [alpha]})\n",
    "    clue_board_df[\"Model\"] = \"swow\"\n",
    "    clue_board_df[\"type\"] = row[\"type\"]\n",
    "    clue_board_df[\"n_steps\"] = row[\"n_steps\"]        \n",
    "    clue_board_df[\"boardnames\"] = boardname\n",
    "    clue_board_df[\"wordpair\"] = wordpair\n",
    "    clue_board_df[\"cluelist\"] = str(','.join(cluelist))\n",
    "    clue_board_df[\"clue_score\"] = str(np.round(mainscores,10).tolist())\n",
    "      \n",
    "    candidateprobs_nonRSA = pd.concat([candidateprobs_nonRSA, clue_board_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4_h0-Fw_xol"
   },
   "source": [
    "### obtaining probabilities for data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ri3s5WvW_0hn"
   },
   "outputs": [],
   "source": [
    "#df_filtered[\"wordpair\"] = df_filtered[\"Word1\"]+\"-\"+df_filtered[\"Word2\"]\n",
    "# TODO: this is slow!\n",
    "nonrsa_probs = pd.DataFrame()\n",
    "\n",
    "for index, row in expdata.iterrows():\n",
    "  wordpair = row[\"wordpair\"].replace(\" - \", \"-\")\n",
    "  clue = row[\"Clue1\"]\n",
    "  clue_df = df_filtered[(df_filtered[\"vocab_word\"] == clue) & (df_filtered[\"wordpair\"] == wordpair)]\n",
    "  for i, j in clue_df[:2].iterrows():\n",
    "    ctype = j[\"type\"]\n",
    "    n_steps = j[\"n_steps\"]\n",
    "    clueprobs_df = candidateprobs_nonRSA[(candidateprobs_nonRSA[\"wordpair\"]== wordpair) & (candidateprobs_nonRSA[\"type\"] == ctype) & (candidateprobs_nonRSA[\"n_steps\"]==n_steps)]\n",
    "    for x, y in clueprobs_df.iterrows():\n",
    "      alpha = y[\"alpha\"]\n",
    "      cluelist = y[\"cluelist\"].split(',')\n",
    "      clue_index = cluelist.index(clue)\n",
    "      clue_score = y[\"clue_score\"][1:-1].split(', ')[clue_index]\n",
    "\n",
    "      clue_board_df = pd.DataFrame({'alpha': [alpha]})\n",
    "      clue_board_df[\"type\"] = ctype\n",
    "      clue_board_df[\"n_steps\"] = n_steps\n",
    "      clue_board_df[\"wordpair\"] = wordpair\n",
    "      clue_board_df[\"Clue1\"] = clue\n",
    "      clue_board_df[\"clue_score\"] = clue_score\n",
    "        \n",
    "      nonrsa_probs = pd.concat([nonrsa_probs, clue_board_df])\n",
    "\n",
    "#nonrsa_probs.to_csv(parentfolder+'finalexpdata_nonRSA_speaker.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dQiVTd1wDb-k"
   },
   "source": [
    "## RSA (fixed walks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eH-KcyesGuJT"
   },
   "outputs": [],
   "source": [
    "rsa_optimal_params = {\n",
    "    'swow' : (25.1522030761838, 0.03863169001849234),\n",
    "    'glove' : (82.83019661384789, 0.9997249702731884),\n",
    "    'bert-sum' : (29.709602301411962, 0.031659060110267576), #-17533\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "scM-hPvIKUSm"
   },
   "outputs": [],
   "source": [
    "parentfolder = \"/content/drive/My Drive/search-models/\"\n",
    "pragmaticspeaker_df = pd.read_csv(parentfolder+'candidates_RSAprobs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DlsUEOyaGgrN"
   },
   "outputs": [],
   "source": [
    "## need to obtain list of candidates for each board separately\n",
    "\n",
    "#pragmaticspeaker_df = pd.DataFrame()\n",
    "modelname = 'swow'\n",
    "\n",
    "beta = rsa_optimal_params[modelname][0]\n",
    "cost = rsa_optimal_params[modelname][1]\n",
    "for index, row in cwide.iterrows():\n",
    "  boardname = row[\"boardnames\"]\n",
    "  cluelist = row[\"clue_list\"]\n",
    "  wordpair = row[\"wordpair\"]\n",
    "  clue_probs = search_funcs.RSA.pragmatic_speaker(boardname, beta, cost, representations, 'swow', cluelist, vocab, boards)\n",
    "  ## obtain the probs for the specific wordpair\n",
    "  combos_df = search_funcs.RSA.compute_board_combos(boardname,boards)\n",
    "  wordpairlist = list(combos_df[\"wordpair\"])\n",
    "  mainscores = clue_probs[wordpairlist.index(wordpair)]\n",
    "\n",
    "  clue_board_df = pd.DataFrame({'Model': [modelname]})\n",
    "  clue_board_df[\"boardnames\"] = boardname\n",
    "  clue_board_df[\"type\"] = row[\"type\"]\n",
    "  clue_board_df[\"n_steps\"] = row[\"n_steps\"]   \n",
    "  clue_board_df[\"wordpair\"] = wordpair\n",
    "  clue_board_df[\"cluelist\"] = str(','.join(cluelist))\n",
    "  clue_board_df[\"clue_score\"] = str(np.round(mainscores,10).tolist())\n",
    "    \n",
    "  pragmaticspeaker_df = pd.concat([pragmaticspeaker_df, clue_board_df])\n",
    "  pragmaticspeaker_df.to_csv(parentfolder+'candidates_RSAprobs.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iiRgjHRjDgY7"
   },
   "source": [
    "### obtaining probabilities for data\n",
    "\n",
    "Now that we have the candidate-level probabilities for the pragmatic speaker, we compute the probabilities for the behavioral data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z0pBTT6DFdit"
   },
   "outputs": [],
   "source": [
    "#candidates_df[\"wordpair\"] = candidates_df[\"Word1\"]+\"-\"+candidates_df[\"Word2\"]\n",
    "\n",
    "#rsa_probs = pd.DataFrame()\n",
    "\n",
    "for index, row in expdata.iterrows():\n",
    "  wordpair = row[\"wordpair\"].replace(\" - \", \"-\")\n",
    "  clue = row[\"Clue1\"]\n",
    "  clue_df = df_filtered[(df_filtered[\"vocab_word\"] == clue) & (df_filtered[\"wordpair\"] == wordpair)]\n",
    "  for i, j in clue_df.iterrows():\n",
    "    ctype = j[\"type\"]\n",
    "    n_steps = j[\"n_steps\"]\n",
    "    clueprobs_df = pragmaticspeaker_df[(pragmaticspeaker_df[\"wordpair\"]== wordpair) & (pragmaticspeaker_df[\"type\"] == ctype) & (pragmaticspeaker_df[\"n_steps\"]==n_steps)]\n",
    "    if(len(clueprobs_df)>0):\n",
    "      clue_index = list(clueprobs_df.cluelist)[0].split(',').index(clue)\n",
    "      clue_score = list(clueprobs_df.clue_score)[0][1:-1].split(', ')[clue_index]\n",
    "\n",
    "      clue_board_df = pd.DataFrame({'alpha': [\"RSA\"]})\n",
    "      clue_board_df[\"type\"] = ctype\n",
    "      clue_board_df[\"n_steps\"] = n_steps\n",
    "      clue_board_df[\"wordpair\"] = wordpair\n",
    "      clue_board_df[\"Clue1\"] = clue\n",
    "      clue_board_df[\"clue_score\"] = clue_score\n",
    "        \n",
    "      rsa_probs = pd.concat([rsa_probs, clue_board_df])\n",
    "      rsa_probs.to_csv(parentfolder+'finalexpdata_RSA_speaker.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rOZASDhhNiZv"
   },
   "outputs": [],
   "source": [
    "rsa_probs.to_csv(parentfolder+'finalexpdata_RSA_speaker.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4zOJOeWyc2XF"
   },
   "source": [
    "# Comparing with online candidates study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oZVIHwIkc4zX"
   },
   "outputs": [],
   "source": [
    "parentfolder = \"/content/drive/My Drive/search-models/\"\n",
    "online = pd.read_csv(parentfolder+'online_new_coded.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kQpRbWg4eSSa"
   },
   "outputs": [],
   "source": [
    "main_online = online[[\"wordpair_id\", \"Level\", \"clueOption1\", \"clueOption2\", \"clueOption3\", \"clueOption4\", \"clueOption5\", \"clueOption6\", \"clueOption7\", \"clueOption8\"]]\n",
    "main_online = main_online.melt(id_vars=['wordpair_id', 'Level'], value_vars=[\"clueOption1\", \"clueOption2\", \"clueOption3\", \"clueOption4\", \"clueOption5\", \"clueOption6\", \"clueOption7\", \"clueOption8\"])\n",
    "main_online = main_online.dropna()\n",
    "main_online = main_online.drop_duplicates()\n",
    "main_online = main_online.groupby(['wordpair_id', 'variable', 'Level'], as_index=False)['value'].agg(','.join)\n",
    "main_online['clue_list'] = main_online['value'].str.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kox0BAcEfAbU"
   },
   "outputs": [],
   "source": [
    "common_candidates = pd.DataFrame()\n",
    "\n",
    "for index, row in main_online.iterrows():\n",
    "  wordpair = row[\"wordpair_id\"]\n",
    "  clue_list = row[\"clue_list\"]\n",
    "  # find all candidates for that wordpair in cwide\n",
    "  wp_candidates = cwide[cwide[\"wordpair\"] == wordpair]\n",
    "  if(len(wp_candidates) == 0):\n",
    "    w1, w2 = wordpair.split(\"-\")\n",
    "    wordpair = w2 + \"-\" + w1\n",
    "    wp_candidates = cwide[cwide[\"wordpair\"] == wordpair]\n",
    "    \n",
    "  for i, j in wp_candidates.iterrows():\n",
    "    candidate_list = j[\"clue_list\"]\n",
    "    intersection = list(set(clue_list).intersection(candidate_list))\n",
    "\n",
    "    clue_board_df = pd.DataFrame({'wordpair': [wordpair]})\n",
    "    clue_board_df[\"Level\"] = row[\"Level\"]\n",
    "    clue_board_df[\"candidate_type\"] = row[\"variable\"]\n",
    "    clue_board_df[\"type\"] = j[\"type\"]\n",
    "    clue_board_df[\"n_steps\"] = j[\"n_steps\"]\n",
    "    clue_board_df[\"n_model_candidates\"] = len(candidate_list)\n",
    "    clue_board_df[\"n_human_candidates\"] = len(clue_list)\n",
    "    clue_board_df[\"n_intersection\"] = len(intersection)\n",
    "    clue_board_df[\"intersection\"] = str(intersection)\n",
    "      \n",
    "    common_candidates = pd.concat([common_candidates, clue_board_df])\n",
    "\n",
    "common_candidates.to_csv(parentfolder+'common_candidates.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iuxZRpUSYSyv"
   },
   "source": [
    "# Writing files to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ALztsNDTyVEY"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive',force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "woMZx6ojybl5"
   },
   "outputs": [],
   "source": [
    "parentfolder = \"/content/drive/My Drive/search-models/\"\n",
    "common_candidates.to_csv(parentfolder+'common_candidates.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qt6jgRIeM7YO"
   },
   "source": [
    "# Old: Dedicated functions for full vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "5gKTDYETeKlH"
   },
   "outputs": [],
   "source": [
    "## create boards and merge with expdata\n",
    "combined_boards_df = pd.DataFrame(columns=['Experiment', 'Board','boardwords'])\n",
    "combined_boards_df[\"Experiment\"]  = [\"E1\"] * 10 + [\"E2\"] * 10\n",
    "combined_boards_df[\"Board\"] = [\"TrialList\" + str(i) for i in range(1,11)] * 2\n",
    "combined_boards_df[\"boardnames\"] = (['e1_board' + str(i) + '_words' for i in range(1,11)] \n",
    "                                  + ['e2_board' + str(i) + '_words' for i in range(1,11)])\n",
    "combined_boards_df[\"boardwords\"] = [boards[n] for n in combined_boards_df[\"boardnames\"]]\n",
    "\n",
    "expdata_new = pd.merge(expdata,combined_boards_df,on=['Board', 'Experiment'],how='left')\n",
    "expdata_new[\"wordpair\"] = expdata_new[\"Word1\"] + \"-\" + expdata_new[\"Word2\"]\n",
    "board_combos = {board_name : search_funcs.RSA.compute_board_combos(board_name, boards) for board_name in boards.keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FZOYctgjM5NM"
   },
   "source": [
    "## Non - RSA method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "v8QEomkF4hmB"
   },
   "outputs": [],
   "source": [
    "board_optimal_params = {\n",
    "    'swow' : (23.488850322875496, 1), # -13204\n",
    "    'glove' : (20.952928531665275, 1), # -15774.814774380024)\n",
    "    'bert-sum' : (19.983835225540847, 0.787924454045298),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "9r7kmGrcNCvB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for swow and alpha 0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cluescoredf \u001b[38;5;241m=\u001b[39m \u001b[43msearch_funcs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnonRSA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspeaker_targetboard_cluescores\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mswow\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mglove\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboard_optimal_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboard_combos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboards\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_word\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepresentations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpdata_new\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m cluescoredf\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m~/Repos/Connector/search-models/search_funcs.py:677\u001b[0m, in \u001b[0;36mnonRSA.speaker_targetboard_cluescores\u001b[0;34m(modelnames, optimal_params, board_combos, boards, candidates, vocab, representations, target_df, cluedata)\u001b[0m\n\u001b[1;32m    674\u001b[0m beta \u001b[38;5;241m=\u001b[39m optimal_params[modelname][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodelname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and alpha \u001b[39m\u001b[38;5;132;01m{\u001b[39;00malpha\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 677\u001b[0m speaker_board_probs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    678\u001b[0m     board_name : nonRSA\u001b[38;5;241m.\u001b[39mspeaker_targetboard(boards[board_name], alpha, beta, candidates, representations, modelname, vocab, target_df)\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m board_name \u001b[38;5;129;01min\u001b[39;00m boards\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[1;32m    680\u001b[0m }   \n\u001b[1;32m    682\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m board \u001b[38;5;129;01min\u001b[39;00m speaker_board_probs\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    683\u001b[0m   \n\u001b[1;32m    684\u001b[0m   \u001b[38;5;66;03m## get the clues we need scores for from expdatanew\u001b[39;00m\n\u001b[1;32m    685\u001b[0m   clue_main \u001b[38;5;241m=\u001b[39m cluedata\u001b[38;5;241m.\u001b[39mloc[cluedata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboardnames\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m board]\n",
      "File \u001b[0;32m~/Repos/Connector/search-models/search_funcs.py:678\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    674\u001b[0m beta \u001b[38;5;241m=\u001b[39m optimal_params[modelname][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodelname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and alpha \u001b[39m\u001b[38;5;132;01m{\u001b[39;00malpha\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    677\u001b[0m speaker_board_probs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m--> 678\u001b[0m     board_name : \u001b[43mnonRSA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspeaker_targetboard\u001b[49m\u001b[43m(\u001b[49m\u001b[43mboards\u001b[49m\u001b[43m[\u001b[49m\u001b[43mboard_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcandidates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepresentations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodelname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m board_name \u001b[38;5;129;01min\u001b[39;00m boards\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[1;32m    680\u001b[0m }   \n\u001b[1;32m    682\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m board \u001b[38;5;129;01min\u001b[39;00m speaker_board_probs\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    683\u001b[0m   \n\u001b[1;32m    684\u001b[0m   \u001b[38;5;66;03m## get the clues we need scores for from expdatanew\u001b[39;00m\n\u001b[1;32m    685\u001b[0m   clue_main \u001b[38;5;241m=\u001b[39m cluedata\u001b[38;5;241m.\u001b[39mloc[cluedata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboardnames\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m board]\n",
      "File \u001b[0;32m~/Repos/Connector/search-models/search_funcs.py:628\u001b[0m, in \u001b[0;36mnonRSA.speaker_targetboard\u001b[0;34m(context_board, alpha, beta, candidates, representations, modelname, vocab, target_df)\u001b[0m\n\u001b[1;32m    623\u001b[0m board_vectors \u001b[38;5;241m=\u001b[39m representations[modelname][board_word_indices]\n\u001b[1;32m    625\u001b[0m \u001b[38;5;66;03m## clue_sims is the similarity of ALL clues in full searchspace (size N) to EACH word on board (size 20)\u001b[39;00m\n\u001b[1;32m    626\u001b[0m \n\u001b[1;32m    627\u001b[0m \u001b[38;5;66;03m### NEED TO FIX THIS TO ONLY CONSIDER CANDIDATES!!\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m candidate_index \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlist\u001b[39m(vocab[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocab_word\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mindex(w) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m candidates]\n\u001b[1;32m    629\u001b[0m candidate_embeddings \u001b[38;5;241m=\u001b[39m representations[modelname][candidate_index]\n\u001b[1;32m    631\u001b[0m clue_sims \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mscipy\u001b[38;5;241m.\u001b[39mspatial\u001b[38;5;241m.\u001b[39mdistance\u001b[38;5;241m.\u001b[39mcdist(board_vectors, candidate_embeddings, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcosine\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m ) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "File \u001b[0;32m~/Repos/Connector/search-models/search_funcs.py:628\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    623\u001b[0m board_vectors \u001b[38;5;241m=\u001b[39m representations[modelname][board_word_indices]\n\u001b[1;32m    625\u001b[0m \u001b[38;5;66;03m## clue_sims is the similarity of ALL clues in full searchspace (size N) to EACH word on board (size 20)\u001b[39;00m\n\u001b[1;32m    626\u001b[0m \n\u001b[1;32m    627\u001b[0m \u001b[38;5;66;03m### NEED TO FIX THIS TO ONLY CONSIDER CANDIDATES!!\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m candidate_index \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvocab_word\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mindex(w) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m candidates]\n\u001b[1;32m    629\u001b[0m candidate_embeddings \u001b[38;5;241m=\u001b[39m representations[modelname][candidate_index]\n\u001b[1;32m    631\u001b[0m clue_sims \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mscipy\u001b[38;5;241m.\u001b[39mspatial\u001b[38;5;241m.\u001b[39mdistance\u001b[38;5;241m.\u001b[39mcdist(board_vectors, candidate_embeddings, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcosine\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m ) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cluescoredf = search_funcs.nonRSA.speaker_targetboard_cluescores(['swow', 'glove'], board_optimal_params, board_combos, boards, list(vocab.vocab_word), vocab, representations, target_df, expdata_new)\n",
    "cluescoredf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67827P5C897A"
   },
   "source": [
    "## RSA Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IVrhPE-xN6I4"
   },
   "outputs": [],
   "source": [
    "rsa_optimal_params = {\n",
    "    'swow' : (25.1522030761838, 0.03863169001849234),\n",
    "    'glove' : (22.336514544537227, 0.039),\n",
    "    'bert-sum' : (29.709602301411962, 0.031659060110267576), #-17533\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ewq3Y68zSd_H"
   },
   "outputs": [],
   "source": [
    "pragmaticspeakerdf = search_funcs.RSA.get_speaker_df(representations, combined_boards_df,rsa_optimal_params, list(vocab.vocab_word), vocab, expdata_new, board_combos, target_df, boards)\n",
    "pragmaticspeakerdf.head()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPN3gA5HcesL1IibYO9jUBA",
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "search-models.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
